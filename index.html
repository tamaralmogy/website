<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-54246025-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-54246025-1');
  </script>

  <title>Tamar @ RUni</title>

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="180x180" href="/favicons/v3/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicons/v3/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicons/v3/favicon-16x16.png">
  <link rel="manifest" href="/favicons/v3/site.webmanifest">

  <meta charset="UTF-8">
  <meta name="description" content="Assistant Professor, Reichman University. Works at the intersection of computer graphics, computer vision, and HCI, with focus on tools, algorithms, and new paradigms for photo and video editing.">
  <meta name="keywords" content="ohad,fried,ohad fried,research,IDC Herzliya,Reichman University,stanford,graphics,computer graphics,vision,computer vision,computer science,HCI,human computer interaction,neural rendering,deepfakes,perspective distortion,distractors,image distractors,selfie effect,patch2vec,princeton,portraits,photo recoloring,IsoMatch,AudioQuilt,sound mapping,texture synthesis,brown institute,siebel,adobe,google,SIGGRAPH">
  <meta name="author" content="Ohad Fried">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="all">

  <!-- The following pattern defers CSS loading, making page load much faster -->
  <link rel="preload" href="https://code.jquery.com/ui/1.12.1/themes/smoothness/jquery-ui.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href="https://code.jquery.com/ui/1.12.1/themes/smoothness/jquery-ui.css"></noscript>

  <script src="https://code.jquery.com/jquery-3.6.3.min.js" defer></script>
  <script src="https://code.jquery.com/ui/1.13.2/jquery-ui.min.js" defer></script>

  <link rel="stylesheet" type="text/css" href="cv.css">
  <link rel="stylesheet" type="text/css" href="nav.css">

  <!-- Use FontAwesome for (most) icons -->
  <script src="https://kit.fontawesome.com/2a7f8c9d6d.js" defer crossorigin="anonymous"></script>

  <!-- Use AcademicIcons for Google Scholar and Semantic Scholar icons -->
  <!-- The following pattern defers CSS loading, making page load much faster -->
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"></noscript>

  <script src="logic.js" defer></script>
  <script src="no-transition-on-resize.js" defer></script>

  <!--
  <script>
    // Google Analytics
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-54246025-1', 'auto');
    ga('require', 'displayfeatures');
    ga('send', 'pageview');
  </script>
  -->

  <script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Person",
    "name": "Ohad Fried",
    "url": "http://www.ohadf.com",
    "sameAs": [
      "https://www.linkedin.com/in/ohadfried/",
      "https://www.facebook.com/ohadfried",
      "https://www.youtube.com/user/ohadfried/",
      "https://twitter.com/ohadf"
    ]
  }
  </script>
</head>

<body>

  <header>
    <div class="logo">
      <img id="me" loading=lazy alt="A photo of Tamar Almogy" src="./assets/תמונה.jpg">
      <a href="#">Tamar Almogy</a>
    <input type="checkbox" id="nav-toggle" class="nav-toggle" aria-label="Toggle navigation bar">
    <nav>
      <ul>
        <li><a href="#" aria-label="home"><i class="fa-solid fa-house" aria-hidden="true"></i></a></li>
        <li><a href="#about">About</a></li>
        <li><a href="#projects">Projects</a></li>
        <li><a href="#traveling">Traveling</a></li>
        <li><a href="#hobbies">Hobbies</a></li>
        <li><a href="#contact">Contact</a></li>

        <li><a href="mailto:tamar.almogy@post.runi.ac.il" aria-label="email"><i class="fa-solid fa-envelope" aria-hidden="true"></i></a></li>
      </ul>
    </nav>
    <label for="nav-toggle" class="nav-toggle-label">
      <span>
        <i class="fa fa-bars fa-lg"></i>
      </span>
    </label>
  </header>

  <div id="page-wrap">

  <div class="clear"></div>
  <div class="clear"></div>

  <section id="profile">
    <div class="section__pic-container">
      <img alt="A photo of Tamar Almogy" src="./assets/tamar_logo.png">
    </div>
    <div class="section__text">
      <p class="section__text__p1">Hello, I'm</p>
      <h1 class="title">Tamar Almogy</h1>
      <p class="section__text__p2">and welcome to my page!</p>
      <div class="btn-container">
        <button
          class="btn btn-color-1"
          onclick="window.open('./assets/TamarAlmogy-CV.pdf')"
        >
          Download CV
      </div>
      <div id="socials-container">
        <img
          src="./assets/linkedin.png"
          alt="My LinkedIn profile"
          class="icon"
          onclick="location.href='https://www.linkedin.com/in/tamar-almogy-936004223/'"
        />
        <img
          src="./assets/github.png"
          alt="My Github profile"
          class="icon"
          onclick="location.href='https://github.com/tamaralmogy'"
        />
      </div>
    </div>
  </section>

  <div class="clear"></div>

  <section id="about">
    <h2 class="title" id="about">About Me</h2>
    <div class="text-container">
      <p>
        I am a third-year student of computer science at <a href="https://www.runi.ac.il/en" target="_blank" rel="noopener">Reichman University</a>.
        I live in Tel-Aviv for the past four years. However, my roots trace back to <a href="https://www.google.com/maps/place/%D7%9E%D7%91%D7%A9%D7%A8%D7%AA+%D7%A6%D7%99%D7%95%D7%9F%E2%80%AD/@31.7966965,35.1518804,14z/data=!3m1!4b1!4m6!3m5!1s0x1502d6ef90670629:0x69762a21d65be083!8m2!3d31.804052!4d35.157891!16zL20vMDQ5cnMx?entry=ttu"
        target="_blank" rel="noopener">Mevaseret Zion</a>, where I spent my formative years.
        My childhood was enriched by experiences beyond borders. Over the span of four years, my family embarked on a journey to the United States,
        driven by the professional pursuits of my parents. Those years abroad ignited a sense of curiosity and adventure within me, laying the foundation for a global perspective
        that continues to shape my worldview. <br>
        Family holds a special place in my heart, with one brother and two sisters who have been my companions through life's twists and turns. <br>
        Venturing into the realm of coding has been a recent fascination for me. 
        While it's a new terrain, I'm gradually unraveling its intricacies and finding joy in the elegant solutions it offers.
      </p>
    </div>
    <div id="images-container">
      <img
      src="./assets/home.jpeg"
      alt="Home"
      class="home pic"
      />
      <img
      src="./assets/family.jpeg"
      alt="Croatia"
      class="croatia"
      />
      <img
      src="./assets/siblings.jpeg"
      alt="Siblings"
      class="siblings"
      />
      <img
      src="./assets/mom.jpeg"
      alt="My mom"
      class="mom"
      />
    </div>
  </section>

  


  <div class="clear"></div>

  <h2 id="preprints">Preprints</h2>
  <div>
    <div class="grid-container">
      <!--
      <div class="publication_figure">
        <img loading=lazy alt="" src="images/chosen-one.png" srcset="images-responsive/chosen-one-100w.png 100w, images-responsive/chosen-one-200w.png 200w, images-responsive/chosen-one-300w.png 300w, images-responsive/chosen-one-400w.png 400w, images-responsive/chosen-one-500w.png 500w, images-responsive/chosen-one-600w.png 600w, images-responsive/chosen-one-700w.png 700w, images-responsive/chosen-one-800w.png 800w, images-responsive/chosen-one-900w.png 900w, images-responsive/chosen-one-1000w.png 1000w, images-responsive/chosen-one-1100w.png 1100w, images-responsive/chosen-one-1200w.png 1200w, images-responsive/chosen-one-1300w.png 1300w, images-responsive/chosen-one-1400w.png 1400w, images-responsive/chosen-one-1500w.png 1500w, images-responsive/chosen-one-1600w.png 1600w, images-responsive/chosen-one-1700w.png 1700w, images-responsive/chosen-one-1800w.png 1800w, images-responsive/chosen-one-1900w.png 1900w, images-responsive/chosen-one-3446w.png 3446w" sizes="250px">
      </div>
      <div class="publication_text">
        O. Avrahami, A. Hertz, Y. Vinker, M. Arar, S. Fruchter, <b>O. Fried</b>, D. Cohen-Or and D. Lischinski. <br>
        <span class="paper">The Chosen One: Consistent Characters in Text-to-Image Diffusion Models.</span> <br>
        arXiv, 2023. <br>
        <a href = "https://omriavrahami.com/the-chosen-one/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://arxiv.org/pdf/2311.10093" target="_blank" rel="noopener">[pdf]</a>
      </div>
      -->
      <div class="publication_figure">
        <img loading=lazy alt="" src="images/diff-diff.png" srcset="images-responsive/diff-diff-100w.png 100w, images-responsive/diff-diff-200w.png 200w, images-responsive/diff-diff-300w.png 300w, images-responsive/diff-diff-400w.png 400w, images-responsive/diff-diff-500w.png 500w, images-responsive/diff-diff-600w.png 600w, images-responsive/diff-diff-700w.png 700w, images-responsive/diff-diff-800w.png 800w, images-responsive/diff-diff-900w.png 900w, images-responsive/diff-diff-1000w.png 1000w, images-responsive/diff-diff-1100w.png 1100w, images-responsive/diff-diff-1200w.png 1200w, images-responsive/diff-diff-1300w.png 1300w, images-responsive/diff-diff-1400w.png 1400w, images-responsive/diff-diff-1500w.png 1500w, images-responsive/diff-diff-1522w.png 1522w" sizes="250px">
      </div>
      <div class="publication_text">
        E. Levin and <b>O. Fried</b>. <br>
        <span class="paper">Differential Diffusion: Giving Each Pixel Its Strength.</span> <br>
        arXiv, 2023. <br>
        <a href = "https://differential-diffusion.github.io/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://arxiv.org/pdf/2306.00950" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://github.com/exx8/differential-diffusion" target="_blank" rel="noopener">[code]</a>
        <a href = "https://colab.research.google.com/github/exx8/differential-diffusion/blob/main/examples/SD2.ipynb" target="_blank" rel="noopener">[colab]</a>
        <a href = "https://huggingface.co/spaces/exx8/differential-diffusion" target="_blank" rel="noopener">[demo]</a>
      </div>

    </div>
  </div>

  <div class="clear"></div>

  <h2 id="publications">Publications</h2>
  <div>
    <div class="grid-container">

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/taming.png" srcset="images-responsive/taming-100w.png 100w, images-responsive/taming-200w.png 200w, images-responsive/taming-300w.png 300w, images-responsive/taming-400w.png 400w, images-responsive/taming-500w.png 500w, images-responsive/taming-600w.png 600w, images-responsive/taming-700w.png 700w, images-responsive/taming-800w.png 800w, images-responsive/taming-900w.png 900w, images-responsive/taming-1570w.png 1570w" sizes="250px">
      </div>
      <div class="publication_text">
        S. Malnick, S. Avidan and <b>O. Fried</b>. <br>
        <span class="paper">Taming Normalizing Flows.</span> <br>
        WACV, 2024. <br>
        <a href = "https://shimonmalnick.github.io/GenTame/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://arxiv.org/pdf/2211.16488" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://github.com/ShimonMalnick/Taming_a_Generative_Model" target="_blank" rel="noopener">[code]</a>
        <!-- <span title="" id="" class="bib">[bib]</span> -->
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/DIF.jpg" srcset="images-responsive/DIF-100w.jpg 100w, images-responsive/DIF-200w.jpg 200w, images-responsive/DIF-300w.jpg 300w, images-responsive/DIF-400w.jpg 400w, images-responsive/DIF-500w.jpg 500w, images-responsive/DIF-600w.jpg 600w, images-responsive/DIF-700w.jpg 700w, images-responsive/DIF-800w.jpg 800w, images-responsive/DIF-900w.jpg 900w, images-responsive/DIF-1216w.jpg 1216w" sizes="250px">
      </div>
      <div class="publication_text">
        S. Sinitsa and <b>O. Fried</b>. <br>
        <span class="paper">Deep Image Fingerprint: Towards Low Budget Synthetic Image Detection and Model Lineage Analysis.</span> <br>
        WACV, 2024. <br>
        <a href = "https://sergo2020.github.io/DIF/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://arxiv.org/pdf/2303.10762" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://github.com/Sergo2020/DIF_pytorch_official" target="_blank" rel="noopener">[code]</a>
        <!-- <span title="" id="" class="bib">[bib]</span> -->
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/reface.png" srcset="images-responsive/reface-100w.png 100w, images-responsive/reface-200w.png 200w, images-responsive/reface-300w.png 300w, images-responsive/reface-400w.png 400w, images-responsive/reface-500w.png 500w, images-responsive/reface-600w.png 600w, images-responsive/reface-700w.png 700w, images-responsive/reface-800w.png 800w, images-responsive/reface-900w.png 900w, images-responsive/reface-2772w.png 2772w" sizes="250px">
      </div>
      <div class="publication_text">
        D. Arkushin, B. Cohen, S. Peleg and <b>O. Fried</b>. <br>
        <span class="paper">GEFF: Improving Any Clothes-Changing Person ReID Model using Gallery Enrichment with Face Features.</span> <br>
        RWS Workshop, WACV, 2024. <br>
        <a href = "https://www.vision.huji.ac.il/reface/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://arxiv.org/pdf/2211.13807" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://github.com/bar371/ReFace" target="_blank" rel="noopener">[code]</a>
      </div>

      <div id="link-diffusing-colors" class="publication_figure">
        <img loading=lazy alt="" src="images/diffusing-colors.png" srcset="images-responsive/diffusing-colors-100w.png 100w, images-responsive/diffusing-colors-200w.png 200w, images-responsive/diffusing-colors-300w.png 300w, images-responsive/diffusing-colors-400w.png 400w, images-responsive/diffusing-colors-500w.png 500w, images-responsive/diffusing-colors-600w.png 600w, images-responsive/diffusing-colors-700w.png 700w, images-responsive/diffusing-colors-800w.png 800w, images-responsive/diffusing-colors-900w.png 900w, images-responsive/diffusing-colors-2036w.png 2036w" sizes="250px">
      </div>
      <div class="publication_text">
        N. Zabari, A. Azulay, A. Gorkor, T. Halperin and <b>O. Fried</b>. <br>
        <span class="paper">Diffusing Colors: Image Colorization with Text Guided Diffusion.</span> <br>
        SIGGRAPH Asia, 2023. <br>
        <a href = "https://pub.res.lightricks.com/diffusing-colors/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://arxiv.org/pdf/2312.04145" target="_blank" rel="noopener">[pdf]</a>
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/break-a-scene.png" srcset="images-responsive/break-a-scene-100w.png 100w, images-responsive/break-a-scene-200w.png 200w, images-responsive/break-a-scene-300w.png 300w, images-responsive/break-a-scene-400w.png 400w, images-responsive/break-a-scene-500w.png 500w, images-responsive/break-a-scene-600w.png 600w, images-responsive/break-a-scene-700w.png 700w, images-responsive/break-a-scene-800w.png 800w, images-responsive/break-a-scene-900w.png 900w, images-responsive/break-a-scene-2676w.png 2676w" sizes="250px">
      </div>
      <div class="publication_text">
        O. Avrahami, K. Aberman, <b>O. Fried</b>, D. Cohen-Or and D. Lischinski. <br>
        <span class="paper">Break-A-Scene: Extracting Multiple Concepts from a Single Image.</span> <br>
        SIGGRAPH Asia, 2023. <br>
        <a href = "https://omriavrahami.com/break-a-scene/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://omriavrahami.com/break-a-scene/static/paper/Break-A-Scene.pdf" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://github.com/google/break-a-scene" target="_blank" rel="noopener">[code]</a>
        <a href = "https://www.youtube.com/watch?v=-9EA-BhizgM" target="_blank" rel="noopener">[video]</a>
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/blended-latent-diffusion.png" srcset="images-responsive/blended-latent-diffusion-100w.png 100w, images-responsive/blended-latent-diffusion-200w.png 200w, images-responsive/blended-latent-diffusion-300w.png 300w, images-responsive/blended-latent-diffusion-400w.png 400w, images-responsive/blended-latent-diffusion-500w.png 500w, images-responsive/blended-latent-diffusion-600w.png 600w, images-responsive/blended-latent-diffusion-700w.png 700w, images-responsive/blended-latent-diffusion-800w.png 800w, images-responsive/blended-latent-diffusion-900w.png 900w, images-responsive/blended-latent-diffusion-2676w.png 2676w" sizes="250px">
      </div>
      <div class="publication_text">
        O. Avrahami, <b>O. Fried</b> and D. Lischinski. <br>
        <span class="paper">Blended Latent Diffusion.</span> <br>
        ACM Transactions on Graphics (Proc. SIGGRAPH), 2023. <br>
        <a href = "https://omriavrahami.com/blended-latent-diffusion-page/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://arxiv.org/pdf/2206.02779" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://github.com/omriav/blended-latent-diffusion" target="_blank" rel="noopener">[code]</a>
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/spatext-small.png" srcset="images-responsive/spatext-small-100w.png 100w, images-responsive/spatext-small-200w.png 200w, images-responsive/spatext-small-300w.png 300w, images-responsive/spatext-small-400w.png 400w, images-responsive/spatext-small-500w.png 500w, images-responsive/spatext-small-600w.png 600w, images-responsive/spatext-small-700w.png 700w, images-responsive/spatext-small-800w.png 800w, images-responsive/spatext-small-900w.png 900w, images-responsive/spatext-small-1284w.png 1284w" sizes="250px">
      </div>
      <div class="publication_text">
        O. Avrahami, T. Hayes, O. Gafni, S. Gupta, Y. Taigman, D. Parikh, D. Lischinski, <b>O. Fried</b> and X. Yin. <br>
        <span class="paper">SpaText: Spatio-Textual Representation for Controllable Image Generation.</span> <br>
        IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. <br>
        <a href = "https://omriavrahami.com/spatext/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://arxiv.org/pdf/2211.14305" target="_blank" rel="noopener">[pdf]</a>
        <!-- <span title="" id="" class="bib">[bib]</span> -->
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/ham2pose.png" srcset="images-responsive/ham2pose-100w.png 100w, images-responsive/ham2pose-200w.png 200w, images-responsive/ham2pose-300w.png 300w, images-responsive/ham2pose-400w.png 400w, images-responsive/ham2pose-500w.png 500w, images-responsive/ham2pose-600w.png 600w, images-responsive/ham2pose-700w.png 700w, images-responsive/ham2pose-800w.png 800w, images-responsive/ham2pose-900w.png 900w, images-responsive/ham2pose-1806w.png 1806w" sizes="250px">
      </div>
      <div class="publication_text">
        R. Shalev-Arkushin, A. Moryossef and <b>O. Fried</b>. <br>
        <span class="paper">Ham2Pose: Animating Sign Language Notation into Pose Sequences.</span> <br>
        IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. <br>
        <a href = "https://rotem-shalev.github.io/ham-to-pose/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://arxiv.org/pdf/2211.13613" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://github.com/rotem-shalev/Ham2Pose" target="_blank" rel="noopener">[code]</a>
        <!-- <span title="" id="" class="bib">[bib]</span> -->
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/ddnerf.png" srcset="images-responsive/ddnerf-100w.png 100w, images-responsive/ddnerf-200w.png 200w, images-responsive/ddnerf-300w.png 300w, images-responsive/ddnerf-400w.png 400w, images-responsive/ddnerf-500w.png 500w, images-responsive/ddnerf-600w.png 600w, images-responsive/ddnerf-700w.png 700w, images-responsive/ddnerf-800w.png 800w, images-responsive/ddnerf-900w.png 900w, images-responsive/ddnerf-2646w.png 2646w" sizes="250px">
      </div>
      <div class="publication_text">
        D. Dadon, <b>O. Fried</b> and Y. Hel-Or. <br>
        <span class="paper">DDNeRF: Depth Distribution Neural Radiance Fields.</span> <br>
        WACV, 2023. <br>
        <a href = "https://dadonda89.github.io/ddnerfweb/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://arxiv.org/pdf/2203.16626.pdf" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://github.com/dadonda89/DDNeRF" target="_blank" rel="noopener">[code]</a>
        <!-- <span title="" id="" class="bib">[bib]</span> -->
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/gan-cocktail.png" srcset="images-responsive/gan-cocktail-100w.png 100w, images-responsive/gan-cocktail-200w.png 200w, images-responsive/gan-cocktail-300w.png 300w, images-responsive/gan-cocktail-400w.png 400w, images-responsive/gan-cocktail-500w.png 500w, images-responsive/gan-cocktail-600w.png 600w, images-responsive/gan-cocktail-700w.png 700w, images-responsive/gan-cocktail-800w.png 800w, images-responsive/gan-cocktail-900w.png 900w, images-responsive/gan-cocktail-3188w.png 3188w" sizes="250px">
      </div>
      <div class="publication_text">
        O. Avrahami, D. Lischinski and <b>O. Fried</b>. <br>
        <span class="paper">GAN Cocktail: mixing GANs without dataset access.</span> <br>
        ECCV, 2022. <br>
        <a href = "https://omriavrahami.com/GAN-cocktail-page/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://arxiv.org/pdf/2106.03847" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://github.com/omriav/GAN-cocktail" target="_blank" rel="noopener">[code]</a>
        <!-- <span title="" id="" class="bib">[bib]</span> -->
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/deep-shadow.png" srcset="images-responsive/deep-shadow-100w.png 100w, images-responsive/deep-shadow-200w.png 200w, images-responsive/deep-shadow-300w.png 300w, images-responsive/deep-shadow-400w.png 400w, images-responsive/deep-shadow-500w.png 500w, images-responsive/deep-shadow-600w.png 600w, images-responsive/deep-shadow-700w.png 700w, images-responsive/deep-shadow-800w.png 800w, images-responsive/deep-shadow-900w.png 900w, images-responsive/deep-shadow-1571w.png 1571w" sizes="250px">
      </div>
      <div class="publication_text">
        A. Karnieli, <b>O. Fried</b> and Y. Hel-Or. <br>
        <span class="paper">DeepShadow: Neural Shape from Shadow.</span> <br>
        ECCV, 2022. <br>
        <a href = "https://asafkar.github.io/deepshadow/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://arxiv.org/pdf/2203.15065.pdf" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://github.com/asafkar/deep_shadow" target="_blank" rel="noopener">[code]</a>
        <!-- <span title="" id="" class="bib">[bib]</span> -->
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/blended-diffusion.png" srcset="images-responsive/blended-diffusion-100w.png 100w, images-responsive/blended-diffusion-200w.png 200w, images-responsive/blended-diffusion-300w.png 300w, images-responsive/blended-diffusion-400w.png 400w, images-responsive/blended-diffusion-500w.png 500w, images-responsive/blended-diffusion-600w.png 600w, images-responsive/blended-diffusion-700w.png 700w, images-responsive/blended-diffusion-800w.png 800w, images-responsive/blended-diffusion-900w.png 900w, images-responsive/blended-diffusion-1569w.png 1569w" sizes="250px">
      </div>
      <div class="publication_text">
        O. Avrahami, D. Lischinski and <b>O. Fried</b>. <br>
        <span class="paper">Blended Diffusion for Text-driven Editing of Natural Images.</span> <br>
        IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <br>
        <a href = "https://omriavrahami.com/blended-diffusion-page/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://arxiv.org/pdf/2111.14818" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://github.com/omriav/blended-diffusion" target="_blank" rel="noopener">[code]</a>
        <!-- <span title="" id="" class="bib">[bib]</span> -->
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/d3dv2.png" srcset="images-responsive/d3dv2-100w.png 100w, images-responsive/d3dv2-200w.png 200w, images-responsive/d3dv2-300w.png 300w, images-responsive/d3dv2-400w.png 400w, images-responsive/d3dv2-500w.png 500w, images-responsive/d3dv2-600w.png 600w, images-responsive/d3dv2-700w.png 700w, images-responsive/d3dv2-800w.png 800w, images-responsive/d3dv2-900w.png 900w, images-responsive/d3dv2-1118w.png 1118w" sizes="250px">
      </div>
      <div class="publication_text">
        A. Tewari, M. Byrasandra Ramalinga, X. Pan, <b>O. Fried</b>, M. Agrawala and C. Theobalt. <br>
        <span class="paper">Disentangled3D: Learning a 3D Generative Model with Disentangled Geometry and Appearance from Monocular Images.</span> <br>
        IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <br>
        <a href = "https://vcai.mpi-inf.mpg.de/projects/D3D/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://arxiv.org/pdf/2203.15926.pdf" target="_blank" rel="noopener">[pdf]</a>
        <!-- <span title="" id="" class="bib">[bib]</span> -->
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/dynamic-guidance.png" srcset="images-responsive/dynamic-guidance-100w.png 100w, images-responsive/dynamic-guidance-200w.png 200w, images-responsive/dynamic-guidance-300w.png 300w, images-responsive/dynamic-guidance-400w.png 400w, images-responsive/dynamic-guidance-500w.png 500w, images-responsive/dynamic-guidance-600w.png 600w, images-responsive/dynamic-guidance-700w.png 700w, images-responsive/dynamic-guidance-800w.png 800w, images-responsive/dynamic-guidance-900w.png 900w, images-responsive/dynamic-guidance-901w.png 901w" sizes="250px">
      </div>
      <div class="publication_text">
        J. L. E, K. Y. Zhai, J. Echevarria, <b>O. Fried</b>, P. Hanrahan and J. A. Landay. <br>
        <span class="paper">Dynamic Guidance for Decluttering Photographic Compositions.</span> <br>
        ACM User Interface Software and Technology Symposium (UIST), 2021. <br>
        <a href = "https://graphics.stanford.edu/projects/declutter/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://www.youtube.com/watch?v=s6YdBHhZkxQ" target="_blank" rel="noopener">[video (presentation)]</a>
        <a href = "https://www.youtube.com/watch?v=_Xq8fO7msP4" target="_blank" rel="noopener">[video (short)]</a>
        <a href = "https://www.youtube.com/watch?v=za_Gw10aLQA" target="_blank" rel="noopener">[video (shorter)]</a>
        <a href = "./papers/EZhaiEchevarriaFriedHanrahanLanday_UIST2021.pdf" target="_blank" rel="noopener">[pdf]</a>
        <!-- <span title="" id="" class="bib">[bib]</span> -->
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/nr-course-siggraph.png" srcset="images-responsive/nr-course-siggraph-100w.png 100w, images-responsive/nr-course-siggraph-200w.png 200w, images-responsive/nr-course-siggraph-300w.png 300w, images-responsive/nr-course-siggraph-400w.png 400w, images-responsive/nr-course-siggraph-500w.png 500w, images-responsive/nr-course-siggraph-600w.png 600w, images-responsive/nr-course-siggraph-700w.png 700w, images-responsive/nr-course-siggraph-800w.png 800w, images-responsive/nr-course-siggraph-900w.png 900w, images-responsive/nr-course-siggraph-1492w.png 1492w" sizes="250px">
      </div>
      <div class="publication_text">
        A. Tewari, <b>O. Fried</b>, J. Thies, V. Sitzmann, S. Lombardi, Z. Xu, T. Simon, M. Nie&szlig;ner, E. Tretschk, L. Liu, B. Mildenhall, P. Srinivasan, R. Pandey, S. Orts-Escolano, S. Fanello, M. Guo, G. Wetzstein, J.-Y. Zhu, C. Theobalt, M. Agrawala, D. B Goldman and M. Zollh&ouml;fer.
        <br>
        <span class="paper">Advances in Neural Rendering.</span> <br>
        ACM SIGGRAPH Courses, 2021. <br>
        <a href = "https://www.neuralrender.com/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://www.youtube.com/watch?v=otly9jcZ0Jg" target="_blank" rel="noopener">[video-1]</a>
        <a href = "https://www.youtube.com/watch?v=aboFl5ozImM" target="_blank" rel="noopener">[video-2]</a>
        <!-- <span title="" id="" class="bib">[bib]</span> -->
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/loops.png" srcset="images-responsive/loops-100w.png 100w, images-responsive/loops-200w.png 200w, images-responsive/loops-300w.png 300w, images-responsive/loops-400w.png 400w, images-responsive/loops-500w.png 500w, images-responsive/loops-600w.png 600w, images-responsive/loops-700w.png 700w, images-responsive/loops-800w.png 800w, images-responsive/loops-900w.png 900w, images-responsive/loops-1268w.png 1268w" sizes="250px">
      </div>
      <div class="publication_text">
        T. Halperin, H. Hakim, O. Vantzos, G. Hochman, N. Benaim, L. Sassy, M. Kupchik, O. Bibi and <b>O. Fried</b>. <br>
        <span class="paper">Endless Loops: Detecting and Animating Periodic Patterns in Still Images.</span> <br>
        ACM Transactions on Graphics (Proc. SIGGRAPH), 2021. <br>
        <a href = "https://pub.res.lightricks.com/endless-loops/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://www.youtube.com/watch?v=8ZYUvxWuD2Y" target="_blank" rel="noopener">[video]</a>
        <a href = "https://medium.com/@lightricks-tech-blog/motion-brush-a-master-stroke-for-the-creator-community-d7e6e19b6eb3" target="_blank" rel="noopener">[blog]</a>
        <!--<a href = "https://arxiv.org/pdf/2105.09374.pdf" target="_blank" rel="noopener">[pdf]</a>-->
        <!--https://storage.googleapis.com/ltx-public-images/Endless_Loops__Detecting_and_animating_periodic_patterns_in_still_images.pdf-->
        <!--<span title="" id="" class="bib">[bib]</span>-->
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/Yao2021.png" srcset="images-responsive/Yao2021-100w.png 100w, images-responsive/Yao2021-200w.png 200w, images-responsive/Yao2021-300w.png 300w, images-responsive/Yao2021-400w.png 400w, images-responsive/Yao2021-500w.png 500w, images-responsive/Yao2021-600w.png 600w, images-responsive/Yao2021-700w.png 700w, images-responsive/Yao2021-800w.png 800w, images-responsive/Yao2021-900w.png 900w" sizes="250px">
      </div>
      <div class="publication_text">
        X. Yao, <b>O. Fried</b>, K. Fatahalian and M. Agrawala. <br>
        <span class="paper">Iterative Text-based Editing of Talking-heads Using Neural Retargeting.</span> <br>
        ACM Transactions on Graphics, 2021. <br>
        <a href = "https://www.davidyao.me/projects/text2vid/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://www.youtube.com/watch?v=oo4tB0f6uqQ" target="_blank" rel="noopener">[video]</a>
        <a href = "./papers/YaoFriedFatahalianAgrawala_TOG2021.pdf" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://www.davidyao.me/projects/text2vid/supplementary_html/index.html" target="_blank" rel="noopener">[supp]</a>
        <!--<span title="" id="bib_yao2021" class="bib">[bib]</span>-->
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/aging.png" srcset="images-responsive/aging-100w.png 100w, images-responsive/aging-200w.png 200w, images-responsive/aging-300w.png 300w, images-responsive/aging-400w.png 400w, images-responsive/aging-500w.png 500w, images-responsive/aging-600w.png 600w, images-responsive/aging-700w.png 700w, images-responsive/aging-800w.png 800w, images-responsive/aging-900w.png 900w, images-responsive/aging-1317w.png 1317w" sizes="250px">
      </div>
      <div class="publication_text">
        R. Or-El, S. Sengupta, <b>O. Fried</b>, E. Shechtman and I. Kemelmacher-Shlizerman. <br>
        <span class="paper">Lifespan Age Transformation Synthesis.</span> <br>
        ECCV, 2020. <br>
        <a href = "https://grail.cs.washington.edu/projects/lifespan_age_transformation_synthesis/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "./papers/OrElSenguptaFriedShechtmanKemelmacherShlizerman_arxiv2020.pdf" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://github.com/royorel/FFHQ-Aging-Dataset" target="_blank" rel="noopener">[dataset]</a>
        <a href = "https://github.com/royorel/Lifespan_Age_Transformation_Synthesis" target="_blank" rel="noopener">[code]</a>
        <a href = "https://colab.research.google.com/github/royorel/Lifespan_Age_Transformation_Synthesis/blob/master/LATS_demo.ipynb" target="_blank" rel="noopener">[colab notebook]</a>
        <span title="" id="bib_orel2020" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/detect-deep-fakes.png" srcset="images-responsive/detect-deep-fakes-100w.png 100w, images-responsive/detect-deep-fakes-200w.png 200w, images-responsive/detect-deep-fakes-300w.png 300w, images-responsive/detect-deep-fakes-400w.png 400w, images-responsive/detect-deep-fakes-500w.png 500w, images-responsive/detect-deep-fakes-600w.png 600w, images-responsive/detect-deep-fakes-700w.png 700w, images-responsive/detect-deep-fakes-800w.png 800w, images-responsive/detect-deep-fakes-900w.png 900w, images-responsive/detect-deep-fakes-1170w.png 1170w" sizes="250px">
      </div>
      <div class="publication_text">
        S. Agarwal, H. Farid, <b>O. Fried</b> and M. Agrawala. <br>
        <span class="paper">Detecting Deep-Fake Videos from Phoneme-Viseme Mismatches.</span> <br>
        CVPR Workshop on Media Forensics, 2020 <br>
        <a href = "./papers/AgarwalFaridFriedAgrawala_CVPRW2020.pdf" target="_blank" rel="noopener">[pdf]</a>
        <span title="" id="bib_agarwal2020" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/neural-rendering.png" srcset="images-responsive/neural-rendering-100w.png 100w, images-responsive/neural-rendering-200w.png 200w, images-responsive/neural-rendering-300w.png 300w, images-responsive/neural-rendering-400w.png 400w, images-responsive/neural-rendering-500w.png 500w, images-responsive/neural-rendering-600w.png 600w, images-responsive/neural-rendering-700w.png 700w, images-responsive/neural-rendering-800w.png 800w, images-responsive/neural-rendering-900w.png 900w, images-responsive/neural-rendering-2348w.png 2348w" sizes="250px">
      </div>
      <div class="publication_text">
        A. Tewari*, <b>O. Fried*</b>, J. Thies*, V. Sitzmann*, S. Lombardi, K. Sunkavalli, R. Martin-Brualla, T. Simon, J. Saragih, M. Nie&szlig;ner, R. Pandey, S. Fanello, G. Wetzstein, J.-Y. Zhu, C. Theobalt, M. Agrawala, E. Shechtman, D. B Goldman and M. Zollh&ouml;fer.
        <small>(* equal contribution)</small> <br>
        <span class="paper">State of the Art on Neural Rendering.</span> <br>
        Computer Graphics Forum (Eurographics STAR report), 2020 <br>
        <a href = "./papers/TewariFriedThiesSitzmannEtAl_EG2020STAR.pdf" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://www.youtube.com/watch?v=0Lt1uVseOsU" target="_blank" rel="noopener">[talk]</a>
        <span title="" id="bib_tewari2020" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/composition-guidance.jpg" srcset="images-responsive/composition-guidance-100w.jpg 100w, images-responsive/composition-guidance-200w.jpg 200w, images-responsive/composition-guidance-300w.jpg 300w, images-responsive/composition-guidance-400w.jpg 400w, images-responsive/composition-guidance-500w.jpg 500w, images-responsive/composition-guidance-600w.jpg 600w, images-responsive/composition-guidance-700w.jpg 700w, images-responsive/composition-guidance-800w.jpg 800w, images-responsive/composition-guidance-900w.jpg 900w, images-responsive/composition-guidance-1290w.jpg 1290w" sizes="250px">
      </div>
      <div class="publication_text">
        J. L. E, <b>O. Fried</b>, J. Lu, J. Zhang, R. Mech, J. Echevarria, P. Hanrahan and J. A. Landay. <br>
        <span class="paper">Adaptive Photographic Composition Guidance.</span> <br>
        CHI, 2020 <br>
        <a href = "https://graphics.stanford.edu/projects/adaptivearmatures/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "./papers/EFriedLuZhangMechEchevarriaHanrahanLanday_CHI2020_small.pdf" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://www.youtube.com/watch?v=bimtzHrXDog" target="_blank" rel="noopener">[video]</a>
        <a href = "https://www.youtube.com/watch?v=mnmHnWuP9Mo" target="_blank" rel="noopener">[talk]</a>
        <span title="" id="bib_e2020" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <!--<img src="images/editing-self-image.png">-->
        <img loading=lazy alt="" src="images/editing-self-image.png" srcset="images-responsive/editing-self-image-100w.png 100w, images-responsive/editing-self-image-200w.png 200w, images-responsive/editing-self-image-300w.png 300w, images-responsive/editing-self-image-400w.png 400w, images-responsive/editing-self-image-500w.png 500w, images-responsive/editing-self-image-600w.png 600w, images-responsive/editing-self-image-678w.png 678w" sizes="250px">
      </div>
      <div class="publication_text">
        <b>O. Fried</b>, J. Jacobs, A. Finkelstein and M. Agrawala. <br>
        <span class="paper">Editing Self Image.</span> <br>
        Communications of the ACM, 2020 <br>
        <a href = "./papers/FriedJacobsFinkelsteinAgrawala_CACM2020.pdf" target="_blank" rel="noopener">[pdf]</a>
        <span title="" id="bib_fried2020a" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <!--<img src="images/text-based.jpg">-->
        <img loading=lazy alt="" src="images/text-based.jpg" srcset="images-responsive/text-based-100w.jpg 100w, images-responsive/text-based-200w.jpg 200w, images-responsive/text-based-300w.jpg 300w, images-responsive/text-based-400w.jpg 400w, images-responsive/text-based-500w.jpg 500w, images-responsive/text-based-600w.jpg 600w, images-responsive/text-based-700w.jpg 700w, images-responsive/text-based-787w.jpg 787w" sizes="250px">
      </div>
      <div class="publication_text">
        <b>O. Fried</b>, A. Tewari, M. Zollho&#776;fer, A. Finkelstein, E. Shechtman, D. B Goldman, K. Genova, Z. Jin, C. Theobalt and M. Agrawala. <br>
        <span class="paper">Text-based Editing of Talking-head Video.</span> <br>
        ACM Transactions on Graphics (Proc. SIGGRAPH), 2019 <br>
        <a href = "./projects/text-based-editing/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "./projects/text-based-editing/data/text-based-editing.pdf" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://www.youtube.com/watch?v=0ybLCfVeFL4" target="_blank" rel="noopener">[video]</a>
        <span title="" id="bib_fried2019b" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <!--<img src="images/portrait_lighting.png">-->
        <img loading=lazy alt="" src="images/portrait_lighting.png" srcset="images-responsive/portrait_lighting-100w.png 100w, images-responsive/portrait_lighting-200w.png 200w, images-responsive/portrait_lighting-300w.png 300w, images-responsive/portrait_lighting-400w.png 400w, images-responsive/portrait_lighting-417w.png 417w" sizes="250px">
      </div>
      <div class="publication_text">
        J. L. E, <b>O. Fried</b> and M. Agrawala. <br>
        <span class="paper">Optimizing Portrait Lighting at Capture-Time Using a 360 Camera as a Light Probe.</span> <br>
        ACM User Interface Software and Technology Symposium (UIST), 2019 <br>
        <a href = "./papers/EFriedAgrawala_UIST2019_small.pdf" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://graphics.stanford.edu/projects/portraitlighting/" target="_blank" rel="noopener">[webpage (supplementary material, visualizations)]</a>
        <a href = "https://www.youtube.com/watch?v=DjQCqmr023k" target="_blank" rel="noopener">[video]</a>
        <a href = "https://www.youtube.com/watch?v=W3miw6cYp80" target="_blank" rel="noopener">[talk]</a>
        <span title="" id="bib_e2019" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <!--<img src="images/puppet_dubbing2_small.png">-->
        <img loading=lazy alt="" src="images/puppet_dubbing2.png" srcset="images-responsive/puppet_dubbing2-100w.png 100w, images-responsive/puppet_dubbing2-200w.png 200w, images-responsive/puppet_dubbing2-300w.png 300w, images-responsive/puppet_dubbing2-400w.png 400w, images-responsive/puppet_dubbing2-500w.png 500w, images-responsive/puppet_dubbing2-600w.png 600w, images-responsive/puppet_dubbing2-700w.png 700w, images-responsive/puppet_dubbing2-800w.png 800w, images-responsive/puppet_dubbing2-816w.png 816w" sizes="250px">
      </div>
      <div class="publication_text">
        <b>O. Fried</b> and M. Agrawala. <br>
        <span class="paper">Puppet Dubbing.</span> <br>
        The 30th Eurographics Symposium on Rendering (EGSR), 2019 <br>
        <a href = "./papers/FriedAgrawala_EGSR2019.pdf" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://youtu.be/SqJQcZ846oU" target="_blank" rel="noopener">[video]</a>
        <a href = "https://drive.google.com/open?id=1tAFBBs0n6s0yjLVCPDOYxbK4GfkLeHlf" target="_blank" rel="noopener">[results]</a>
        <span title="" id="bib_fried2019c" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <!--<img src="images/unsupervised_patches2_small.png">-->
        <img loading=lazy alt="" src="images/unsupervised_patches2.png" srcset="images-responsive/unsupervised_patches2-100w.png 100w, images-responsive/unsupervised_patches2-200w.png 200w, images-responsive/unsupervised_patches2-300w.png 300w, images-responsive/unsupervised_patches2-400w.png 400w, images-responsive/unsupervised_patches2-500w.png 500w, images-responsive/unsupervised_patches2-600w.png 600w, images-responsive/unsupervised_patches2-700w.png 700w, images-responsive/unsupervised_patches2-800w.png 800w, images-responsive/unsupervised_patches2-900w.png 900w, images-responsive/unsupervised_patches2-935w.png 935w" sizes="250px">
      </div>
      <div class="publication_text">
        D. Danon, H. Averbuch-Elor, <b>O. Fried</b> and D. Cohen-Or. <br>
        <span class="paper">Unsupervised Natural Image Patch Learning.</span> <br>
        Computational Visual Media, 2019. <em>Best paper award.</em><br>
        <a href = "./papers/DanonAverbuchElorFriedCohenOr_CVM2019.pdf" target="_blank" rel="noopener">[pdf]</a>
        <span title="" id="bib_danon2019" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <!--<img src="images/selfie_effect.jpg">-->
        <img loading=lazy alt="" src="images/selfie_effect.jpg" srcset="images-responsive/selfie_effect-100w.jpg 100w, images-responsive/selfie_effect-200w.jpg 200w, images-responsive/selfie_effect-300w.jpg 300w, images-responsive/selfie_effect-400w.jpg 400w, images-responsive/selfie_effect-500w.jpg 500w, images-responsive/selfie_effect-600w.jpg 600w, images-responsive/selfie_effect-700w.jpg 700w, images-responsive/selfie_effect-800w.jpg 800w, images-responsive/selfie_effect-900w.jpg 900w, images-responsive/selfie_effect-1970w.jpg 1970w" sizes="250px">
      </div>
      <div class="publication_text">
        B. Ward, M. Ward, <b>O. Fried</b> and B. Paskhover. <br>
        <span class="paper">Nasal Distortion in Short-Distance Photographs: The Selfie Effect.</span> <br>
        JAMA Facial Plastic Surgery, 2018 <br>
        <a href = "./papers/WardWardFriedPaskhover_JAMA2018.pdf" target="_blank" rel="noopener">[pdf]</a>
        <span title="" id="bib_ward2018" class="bib">[bib]</span>

        <br><br>

        <b>O. Fried</b> and B. Paskhover. <br>
        <span class="paper">Perceived Facial Distortions in Selfies Are Explained by Viewing Habits&mdash;Reply.</span> <br>
        JAMA Facial Plastic Surgery, 2018 <br>
        <a href = "./papers/FriedPaskhover_JAMA2018.pdf" target="_blank" rel="noopener">[pdf]</a>
        <span title="" id="bib_fried2018" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <!--<img src="images/patch2vec_small.jpg">-->
        <img loading=lazy alt="" src="images/patch2vec.jpg" srcset="images-responsive/patch2vec-100w.jpg 100w, images-responsive/patch2vec-200w.jpg 200w, images-responsive/patch2vec-300w.jpg 300w, images-responsive/patch2vec-400w.jpg 400w, images-responsive/patch2vec-500w.jpg 500w, images-responsive/patch2vec-600w.jpg 600w, images-responsive/patch2vec-700w.jpg 700w, images-responsive/patch2vec-800w.jpg 800w, images-responsive/patch2vec-900w.jpg 900w, images-responsive/patch2vec-1500w.jpg 1500w" sizes="250px">
      </div>
      <div class="publication_text">
        <b>O. Fried</b>, S. Avidan and D. Cohen-Or. <br>
        <span class="paper">Patch2Vec: Globally Consistent Image Patch Representation.</span> <br>
        Computer Graphics Forum (Proc. Pacific Graphics), 2017 <br>
        <a href = "./papers/FriedAvidanCohenOr_PG2017.pdf " target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://www.youtube.com/watch?v=LBc3snBClYk" target="_blank" rel="noopener">[video]</a>
        <a href = "./data/patch2vec/supp.pdf" target="_blank" rel="noopener">[supp]</a>
        <span title="" id="bib_fried2017b" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <!--<img src="images/princeton.png">-->
        <img loading=lazy alt="" src="images/princeton.png" srcset="images-responsive/princeton-100w.png 100w, images-responsive/princeton-200w.png 200w, images-responsive/princeton-300w.png 300w, images-responsive/princeton-400w.png 400w, images-responsive/princeton-500w.png 500w, images-responsive/princeton-600w.png 600w, images-responsive/princeton-700w.png 700w, images-responsive/princeton-800w.png 800w, images-responsive/princeton-900w.png 900w, images-responsive/princeton-2520w.png 2520w" sizes="250px">
      </div>
      <div class="publication_text">
        <b>O. Fried</b>. <br>
        <span class="paper">Photo Manipulation, The Easy Way.</span> <br>
        PhD Dissertation, Princeton University, 2017
        <a href = "./papers/Fried_Dissertation2017.pdf" target="_blank" rel="noopener">[pdf]</a>
        <span title="" id="bib_fried2017a" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <!--<img src="images/faces_small.png">-->
        <img loading=lazy alt="" src="images/faces.png" srcset="images-responsive/faces-100w.png 100w, images-responsive/faces-200w.png 200w, images-responsive/faces-300w.png 300w, images-responsive/faces-400w.png 400w, images-responsive/faces-500w.png 500w, images-responsive/faces-600w.png 600w, images-responsive/faces-700w.png 700w, images-responsive/faces-800w.png 800w, images-responsive/faces-900w.png 900w, images-responsive/faces-2357w.png 2357w" sizes="250px">
      </div>
      <div class="publication_text">
        <b>O. Fried</b>, E. Shechtman, D. B Goldman and A. Finkelstein. <br>
        <span class="paper">Perspective-aware Manipulation of Portrait Photos.</span> <br>
        ACM Transactions on Graphics (Proc. SIGGRAPH), 2016 <br>
        <a href = "./projects/perspective-portraits/index.html" target="_blank" rel="noopener">[webpage]</a>
        <a href = "./papers/FriedShechtmanGoldmanFinkelstein_SIGGRAPH2016.pdf " target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://www.youtube.com/watch?v=lmXVwEWLwm0" target="_blank" rel="noopener">[video]</a>
        <span title="" id="bib_fried2016" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <!--<img src="images/recolor_small.png">-->
        <img loading=lazy alt="" src="images/recolor.png" srcset="images-responsive/recolor-100w.png 100w, images-responsive/recolor-200w.png 200w, images-responsive/recolor-300w.png 300w, images-responsive/recolor-400w.png 400w, images-responsive/recolor-500w.png 500w, images-responsive/recolor-600w.png 600w, images-responsive/recolor-700w.png 700w, images-responsive/recolor-800w.png 800w, images-responsive/recolor-900w.png 900w, images-responsive/recolor-1584w.png 1584w" sizes="250px">
      </div>
      <div class="publication_text">
        H. Chang, <b>O. Fried</b>, Y. Liu, S. DiVerdi and A. Finkelstein. <br>
        <span class="paper">Palette-based Photo Recoloring.</span> <br>
        ACM Transactions on Graphics (Proc. SIGGRAPH), 2015 <br>
        <a href = "./papers/ChangFriedLiuDiVerdiFinkelstein_SIGGRAPH2015_small.pdf" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://www.youtube.com/watch?v=y30Ey9i9BWQ" target="_blank" rel="noopener">[video]</a>
        <a href = "http://recolor.cs.princeton.edu/demo/index.html" target="_blank" rel="noopener">[demo]</a>
        <a href = "http://gfx.cs.princeton.edu/pubs/Chang_2015_PPR/index.php" target="_blank" rel="noopener">[webpage]</a>
        <span title="" id="bib_chang2015" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <!--<img src="images/distractors.jpg">-->
        <img loading=lazy alt="" src="images/distractors.jpg" srcset="images-responsive/distractors-100w.jpg 100w, images-responsive/distractors-200w.jpg 200w, images-responsive/distractors-300w.jpg 300w, images-responsive/distractors-400w.jpg 400w, images-responsive/distractors-500w.jpg 500w, images-responsive/distractors-600w.jpg 600w, images-responsive/distractors-700w.jpg 700w, images-responsive/distractors-800w.jpg 800w, images-responsive/distractors-900w.jpg 900w, images-responsive/distractors-1200w.jpg 1200w" sizes="250px">
      </div>
      <div class="publication_text">
        <b>O. Fried</b>, E. Shechtman, D. B Goldman and A. Finkelstein. <br>
        <span class="paper">Finding Distractors In Images.</span> <br>
        28th IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015 <br>
        <a href = "./papers/FriedShechtmanGoldmanFinkelstein_CVPR2015.pdf" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://www.youtube.com/watch?v=ZwN1ta-HKjE" target="_blank" rel="noopener">[video]</a>
        <a href = "http://gfx.cs.princeton.edu/pubs/Fried_2015_FDI/index.php" target="_blank" rel="noopener">[webpage]</a>
        <a href = "./data/distractor_removal/distractors_data.zip">[data]</a>
        <a href = "https://github.com/ohadf/distractors">[code]</a>
        <a href = "http://www.popsci.com/software-automatically-takes-out-distracting-parts-your-photos" target="_blank" rel="noopener">[popular-science-article]</a>
        <span title="" id="bib_fried2015b" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <!--<img src="images/isomatch_small.png">-->
        <img loading=lazy alt="" src="images/isomatch.png" srcset="images-responsive/isomatch-100w.png 100w, images-responsive/isomatch-200w.png 200w, images-responsive/isomatch-300w.png 300w, images-responsive/isomatch-400w.png 400w, images-responsive/isomatch-500w.png 500w, images-responsive/isomatch-600w.png 600w, images-responsive/isomatch-700w.png 700w, images-responsive/isomatch-800w.png 800w, images-responsive/isomatch-900w.png 900w, images-responsive/isomatch-1090w.png 1090w" sizes="250px">
      </div>
      <div class="publication_text">
        <b>O. Fried</b>, S. DiVerdi, M. Halber, E. Sizikova and A. Finkelstein. <br>
        <span class="paper">IsoMatch: Creating Informative Grid Layouts.</span> <br>
        36th Annual Conference of the European Association for Computer Graphics (Eurographics), 2015 <br>
        <!--<a href = "./papers/FriedDiVerdiHalberSizikovaFinkelstein_Eurographics2015_HighRes.pdf" target="_blank" rel="noopener">[pdf]</a>-->
        <a href = "./papers/FriedDiVerdiHalberSizikovaFinkelstein_Eurographics2015_small.pdf" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://www.youtube.com/watch?v=WHSAOO8ekM0" target="_blank" rel="noopener">[video]</a>
        <a href = "https://github.com/ohadf/isomatch" target="_blank" rel="noopener">[code]</a>
        <span title="" id="bib_fried2015a" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <!--<img src="images/audioquilt.png">-->
        <img loading=lazy alt="" src="images/audioquilt.png" srcset="images-responsive/audioquilt-100w.png 100w, images-responsive/audioquilt-200w.png 200w, images-responsive/audioquilt-300w.png 300w, images-responsive/audioquilt-400w.png 400w, images-responsive/audioquilt-500w.png 500w, images-responsive/audioquilt-600w.png 600w, images-responsive/audioquilt-700w.png 700w, images-responsive/audioquilt-800w.png 800w, images-responsive/audioquilt-857w.png 857w" sizes="250px">
      </div>
      <div class="publication_text">
        <b>O. Fried</b>, Z. Jin, R. Oda and A. Finkelstein. <br>
        <span class="paper">AudioQuilt: 2D Arrangements of Audio Samples using Metric Learning and Kernelized Sorting.</span> <br>
        14th International Conference on New Interfaces for Musical Expression (NIME), 2014 <br>
        <a href = "./papers/FriedJinOdaFinkelstein_NIME2014.pdf" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://www.youtube.com/watch?v=0GyL271S-aQ" target="_blank" rel="noopener">[video]</a>
        <span title="" id="bib_fried2014" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <!--<img src="images/cross-modal-sound_small.png">-->
        <img loading=lazy alt="" src="images/cross-modal-sound.png" srcset="images-responsive/cross-modal-sound-100w.png 100w, images-responsive/cross-modal-sound-200w.png 200w, images-responsive/cross-modal-sound-300w.png 300w, images-responsive/cross-modal-sound-400w.png 400w, images-responsive/cross-modal-sound-500w.png 500w, images-responsive/cross-modal-sound-600w.png 600w, images-responsive/cross-modal-sound-700w.png 700w, images-responsive/cross-modal-sound-800w.png 800w, images-responsive/cross-modal-sound-900w.png 900w, images-responsive/cross-modal-sound-1267w.png 1267w" sizes="250px">
      </div>
      <div class="publication_text">
        <b>O. Fried</b>, and R. Fiebrink. <br>
        <span class="paper">Cross-modal Sound Mapping Using Deep Learning.</span> <br>
        Proc. New Interfaces for Musical Expression (NIME), 2013 <br>
        <a href = "./papers/FriedFiebrink_NIME2013.pdf" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://www.youtube.com/watch?v=CkdaZFl5sLY" target="_blank" rel="noopener">[video]</a>
        <span title="" id="bib_fried2013" class="bib">[bib]</span>
      </div>

      <div class="publication_figure">
        <!--<img src="images/particles.png">-->
        <img loading=lazy alt="" src="images/particles.png" srcset="images-responsive/particles-100w.png 100w, images-responsive/particles-200w.png 200w, images-responsive/particles-300w.png 300w, images-responsive/particles-400w.png 400w, images-responsive/particles-500w.png 500w, images-responsive/particles-600w.png 600w, images-responsive/particles-700w.png 700w, images-responsive/particles-800w.png 800w, images-responsive/particles-900w.png 900w, images-responsive/particles-1416w.png 1416w" sizes="250px">
      </div>
      <div class="publication_text">
        <b>O. Fried</b>. <br>
        <span class="paper">Texture Synthesis for Particle Infused Textures.</span> <br>
        Masters Thesis, The Hebrew University of Jerusalem, Israel, 2012 <br>
        <a href = "./papers/Fried_MastersThesis2012_small.pdf" target="_blank" rel="noopener">[pdf]</a>
      </div>

      <div class="publication_figure">
        <!--<img src="images/cells_small.png">-->
        <img loading=lazy alt="" src="images/cells.png" srcset="images-responsive/cells-100w.png 100w, images-responsive/cells-200w.png 200w, images-responsive/cells-300w.png 300w, images-responsive/cells-400w.png 400w, images-responsive/cells-500w.png 500w, images-responsive/cells-600w.png 600w, images-responsive/cells-700w.png 700w, images-responsive/cells-800w.png 800w, images-responsive/cells-900w.png 900w, images-responsive/cells-2388w.png 2388w" sizes="250px">
      </div>
      <div class="publication_text">
        <b>O. Fried</b>. <br>
        <span class="paper">Single-cell Resolution Analysis of Yeast Response to Osmo-Stress.</span> <br>
        Undergrad Thesis, The Hebrew University of Jerusalem, Israel, 2010 <br>
      </div>
    </div>
  </div>

  <div class="clear"></div>

  <!--
  <h2 id="preprints">Preprints</h2>
  <div>
    <div class="grid-container">

      <div class="publication_figure">
        <img loading=lazy alt="FakeOut Logo" src="images/fakeout_logo.svg" width="250px">
      </div>
      <div class="publication_text">
        G. Knafo and <b>O. Fried</b>. <br>
        <span class="paper">FakeOut: Leveraging Out-of-domain Self-supervision for Multi-modal Video Deepfake Detection.</span> <br>
        arXiv, 2022. <br>
        <a href = "https://gilikn.github.io/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://arxiv.org/pdf/2212.00773" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://github.com/gilikn/FakeOut" target="_blank" rel="noopener">[code]</a>
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/neural-fonts.jpg" srcset="images-responsive/neural-fonts-100w.jpg 100w, images-responsive/neural-fonts-200w.jpg 200w, images-responsive/neural-fonts-300w.jpg 300w, images-responsive/neural-fonts-400w.jpg 400w, images-responsive/neural-fonts-500w.jpg 500w, images-responsive/neural-fonts-600w.jpg 600w, images-responsive/neural-fonts-700w.jpg 700w, images-responsive/neural-fonts-800w.jpg 800w, images-responsive/neural-fonts-900w.jpg 900w, images-responsive/neural-fonts-4000w.jpg 4000w" sizes="250px">
      </div>
      <div class="publication_text">
        D. Anderson, A. Shamir and <b>O. Fried</b>. <br>
        <span class="paper">Neural Font Rendering.</span> <br>
        arXiv, 2022. <br>
        <a href = "https://arxiv.org/pdf/2211.14802" target="_blank" rel="noopener">[pdf]</a>
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/scene-plausibility.png" srcset="images-responsive/scene-plausibility-100w.png 100w, images-responsive/scene-plausibility-200w.png 200w, images-responsive/scene-plausibility-300w.png 300w, images-responsive/scene-plausibility-400w.png 400w, images-responsive/scene-plausibility-500w.png 500w, images-responsive/scene-plausibility-600w.png 600w, images-responsive/scene-plausibility-700w.png 700w, images-responsive/scene-plausibility-800w.png 800w, images-responsive/scene-plausibility-900w.png 900w, images-responsive/scene-plausibility-3595w.png 3595w" sizes="250px">
      </div>
      <div class="publication_text">
        O. Nachmias, <b>O. Fried</b> and A. Shamir. <br>
        <span class="paper">Prediction of Scene Plausibility.</span> <br>
        arXiv, 2022. <br>
        <a href = "https://arxiv.org/pdf/2212.01470" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://github.com/ornachmias/scene_plausibility" target="_blank" rel="noopener">[code]</a>
      </div>

      <div class="publication_figure">
        <img loading=lazy alt="" src="images/Hughes2021.png" srcset="images-responsive/Hughes2021-100w.png 100w, images-responsive/Hughes2021-200w.png 200w, images-responsive/Hughes2021-300w.png 300w, images-responsive/Hughes2021-400w.png 400w, images-responsive/Hughes2021-500w.png 500w, images-responsive/Hughes2021-600w.png 600w, images-responsive/Hughes2021-700w.png 700w, images-responsive/Hughes2021-800w.png 800w, images-responsive/Hughes2021-900w.png 900w, images-responsive/Hughes2021-1501w.png 1501w" sizes="250px">
      </div>
      <div class="publication_text">
        S. Hughes, <b>O. Fried</b>, M. Ferguson, C. Hughes, R. Hughes, X. Yao and I. Hussey. <br>
        <span class="paper">Deepfaked online content is highly effective in manipulating people's attitudes and intentions.</span> <br>
        PsyArXiv, 2021. <br>
        <a href = "./papers/HughesFriedFergusonHughesHughesYaoHussey_psyarxiv2021.pdf" target="_blank" rel="noopener">[pdf]</a>
        <a href = "https://osf.io/f6ajb/" target="_blank" rel="noopener">[supp]</a>
      </div>

    </div>
  </div>

  <div class="clear"></div>
  -->

  <h2 id="tutorials">Tutorials</h2>
  <div>
    <div class="grid-container">
      <div class="publication_figure">
        <img loading=lazy alt="" src="images/neural-rendering.png" srcset="images-responsive/neural-rendering-100w.png 100w, images-responsive/neural-rendering-200w.png 200w, images-responsive/neural-rendering-300w.png 300w, images-responsive/neural-rendering-400w.png 400w, images-responsive/neural-rendering-500w.png 500w, images-responsive/neural-rendering-600w.png 600w, images-responsive/neural-rendering-700w.png 700w, images-responsive/neural-rendering-800w.png 800w, images-responsive/neural-rendering-900w.png 900w, images-responsive/neural-rendering-2348w.png 2348w" sizes="250px">
      </div>
      <div class="publication_text">
        A. Tewari*, <b>O. Fried*</b>, J. Thies*, V. Sitzmann*, S. Lombardi, K. Sunkavalli, R. Martin-Brualla, T. Simon, J. Saragih, M. Nie&szlig;ner, R. Pandey, S. Fanello, G. Wetzstein, J.-Y. Zhu, C. Theobalt, M. Agrawala, E. Shechtman, D. B Goldman and M. Zollh&ouml;fer.
        <small>(* equal contribution)</small> <br>
        <span class="paper">Neural Rendering.</span> <br>
        CVPR Tutorial, 2020 <br>
        <a href = "https://www.neuralrender.com/" target="_blank" rel="noopener">[webpage]</a>
        <a href = "https://www.youtube.com/watch?v=LCTYRqW-ne8" target="_blank" rel="noopener">[morning session]</a>
        <a href = "https://www.youtube.com/watch?v=JlyGNvbGKB8" target="_blank" rel="noopener">[afternoon session]</a>
        <!--<span title="" id="bib_tewari2020" class="bib">[bib]</span>-->
      </div>
    </div>
  </div>

  <div class="clear"></div>

  <h2 id="media">Media Coverage</h2>
  <div>
    Creating Accessible Solutions To Real-life Problems With Machine Learning.
    <a href="https://issuu.com/tamara.mizroch/docs/herzliyan_2023_final/s/22329630" target="_blank" rel="noopener">[Herzliyan]</a>
    <br><br>

    "Two Minute Papers" highlighted
    <a href="https://www.youtube.com/watch?v=UjuBLS15JqM" target="_blank" rel="noopener">several</a>
    <a href="https://www.youtube.com/watch?v=2xWnOL5bts8" target="_blank" rel="noopener">of our</a>
    <a href="https://www.youtube.com/watch?v=iXqLTJFTUGc" target="_blank" rel="noopener">research</a>
    <a href="https://www.youtube.com/watch?v=BpApq2EPDXE" target="_blank" rel="noopener">projects</a>
    throughout the years.
    <br><br>

    Deepfakes as a force for good (while being mindful of potential misuse).
    <a href="https://www.digitaltrends.com/features/deepfakes-for-good/" target="_blank" rel="noopener">[digitaltrends]</a>
    <br><br>

    Our <a href="https://youtu.be/0ybLCfVeFL4" target="_blank" rel="noopener">YouTube video</a> reached 350,000 views! (For a technical research video with no cats nor babies...) <br><br>
    Our work on Text-based Editing of Talking-head Video (2019) received some media attention. While I would have liked to see
    more on the exciting legitimate use cases, most stories focus on the fake-news angle
    (which is to be expected given the current political climate.) Hopefully this will help highlight the current issues
    with video authentication and more broadly, with misinformation.<br>

    <a href="https://news.stanford.edu/2019/06/05/edit-video-editing-text/" target="_blank" rel="noopener">[Stanford]</a>
    <a href="https://www.washingtonpost.com/news/powerpost/paloma/the-technology-202/2019/06/10/the-technology-202-new-video-editing-technology-raises-disinformation-worries/5cfd7538a7a0a412613357f2/" target="_blank" rel="noopener">[Washington Post]</a>
    <a href="https://www.vice.com/en_us/article/ywyqab/ai-text-editing-talking-head-video-siggraph-2019" target="_blank" rel="noopener">[Vice]</a>
    <a href="https://www.foxnews.com/tech/creepy-deepfake-ai-words-someone-elses-mouth" target="_blank" rel="noopener">[Fox News]</a>
    <a href="https://www.theverge.com/2019/6/10/18659432/deepfake-ai-fakes-tech-edit-video-by-typing-new-words" target="_blank" rel="noopener">[The Verge]</a>
    <a href="https://www.worldstarhiphop.com/videos/video.php?v=wshhQots2bC372mGjfgM" target="_blank" rel="noopener">[WorldStarHipHop]</a>
    <a href="https://mashable.com/article/deepfake-video-altered-text/" target="_blank" rel="noopener">[Mashable]</a>
    <a href="https://www.dailymail.co.uk/sciencetech/article-7115169/AI-lets-edit-people-talking-videos-adding-deleting-words-transcript.html" target="_blank" rel="noopener">[Daily Mail]</a>
    <a href="https://www.theregister.co.uk/2019/06/12/deepfakes_next_generation/" target="_blank" rel="noopener">[The Register]</a>
    <a href="https://www.businessinsider.com/stanford-researchers-technology-alters-speech-in-videos-by-editing-text-2019-6" target="_blank" rel="noopener">[Business Insider]</a>
    <a href="https://thenextweb.com/artificial-intelligence/2019/06/06/new-algorithm-allows-researchers-to-change-what-people-say-on-video-by-editing-transcript/" target="_blank" rel="noopener">[TNW]</a>
    <a href="https://observer.com/2019/06/ai-deepfake-videos-mark-zuckerberg-joe-rogan/" target="_blank" rel="noopener">[Observer]</a>
    <a href="https://www.dazeddigital.com/science-tech/article/45030/1/terrifying-deepfake-tool-lets-you-put-words-in-peoples-mouths-talking-head" target="_blank" rel="noopener">[Dazed]</a>
    <a href="https://www.ynet.co.il/articles/0,7340,L-5523932,00.html" target="_blank" rel="noopener">[ynet (he)]</a>

    and more:
    <div>
      [<a href="https://www.geek.com/tech/deepfake-tool-makes-it-easy-to-put-words-into-someones-mouth-1791338/" target="_blank" rel="noopener"><!--[geek.com]-->1</a>,
      <a href="https://scienceblog.com/508213/editing-video-dialogue-as-easily-as-editing-text/" target="_blank" rel="noopener"><!--[Science Blog]-->2</a>,
      <a href="https://36kr.com/p/5213141" target="_blank" rel="noopener"><!--[36Kr (ch)]-->3</a>,
      <a href="https://www.marketwatch.com/story/stanford-and-princeton-have-figured-out-how-to-edit-video-as-easily-as-text-and-its-as-scary-as-it-sounds-2019-06-06" target="_blank" rel="noopener"><!--[Market Watch]-->4</a>,
      <a href="https://hub.packtpub.com/worried-about-deepfakes-check-out-the-new-algorithm-that-manipulate-talking-head-videos-by-altering-the-transcripts/" target="_blank" rel="noopener"><!--[Packt]-->5</a>,
      <a href="https://www.tomshw.it/altro/video-falsi-ora-unia-fa-dire-cio-che-vuole-alle-persone/" target="_blank" rel="noopener"><!--[Tom's Hardware (it)]-->6</a>,
      <a href="https://news.stanford.edu/press-releases/2019/06/05/edit-video-editing-text/" target="_blank" rel="noopener"><!--[Stanford press release]-->7</a>,
      <a href="http://kenh14.vn/doi-trang-thay-den-thay-doi-loi-noi-cua-con-nguoi-trong-video-bang-cach-chinh-sua-ban-dich-20190607201049858.chn" target="_blank" rel="noopener"><!--[kenh14 (vn)]-->8</a>,
      <a href="https://futurism.com/deepfake-ai-videos-match-transcript-edits" target="_blank" rel="noopener"><!--[Futurism]-->9</a>,
      <a href="https://www.infowars.com/age-of-deep-fakes-algorithm-lets-video-editors-add-change-or-delete-speech/" target="_blank" rel="noopener"><!--[Infowars]-->10</a>,
      <a href="https://www.zukus.net/creepy-deepfake-ai-lets-you-put-words-into-someone-elses-mouth/" target="_blank" rel="noopener"><!--[Zukus]-->11</a>,
      <a href="https://www.globes.co.il/news/article.aspx?did=1001289039" target="_blank" rel="noopener"><!--[Globes (he)]-->12</a>,
      <a href="https://www.sfgate.com/news/article/The-Technology-202-New-video-editing-technology-13964861.php" target="_blank" rel="noopener"><!--[sfgate]-->13</a>,
      <a href="https://www.digitalinformationworld.com/2019/06/text-based-editing-of-talking-head-video.html" target="_blank" rel="noopener"><!--[Digital Information World]-->14</a>,
      <a href="https://www.theinternetofallthings.com/ai-in-video-editing/" target="_blank" rel="noopener"><!--[the internet of all things]-->15</a>,
      <a href="https://www.indiatimes.com/technology/science-and-future/this-ai-lets-someone-edit-footage-of-you-to-make-you-say-what-they-want-just-by-changing-text-368786.html" target="_blank" rel="noopener"><!--[indiatimes]-->16</a>,
      <a href="https://gigazine.net/news/20190610-edit-video-editing-text/" target="_blank" rel="noopener"><!--[Gigazine]-->17</a>,
      <a href="https://www.myscience.org/news/wire/edit_video_by_editing_text-2019-stanford" target="_blank" rel="noopener"><!--[myScience]-->18</a>,
      <a href="https://hipertextual.com/2019/06/ahora-hacer-deepfakes-es-tan-simple-escribir-que-quieres-que-diga" target="_blank" rel="noopener"><!--[hypertextual (sp)]-->19</a>,
      <a href="http://en.brinkwire.com/science/ai-lets-you-edit-people-talking-in-videos-by-adding-or-deleting-words-from-a-transcript/" target="_blank" rel="noopener"><!--[Brinkwire]-->20</a>,
      <a href="https://www.msn.com/he-il/news/other/%D7%94%D7%93%D7%99%D7%A4-%D7%A4%D7%99%D7%99%D7%A7-%D7%9E%D7%A9%D7%AA%D7%9B%D7%9C%D7%9C-%D7%A4%D7%A9%D7%95%D7%98-%D7%AA%D7%A7%D7%9C%D7%99%D7%93%D7%95-%D7%95%D7%94%D7%99%D7%90-%D7%AA%D7%93%D7%91%D7%A8/ar-AACLttA" target="_blank" rel="noopener"><!--[msn (he)]-->21</a>,
      <a href="https://www.mako.co.il/news-channel2/Channel-2-Newscast-q2_2019/Article-9b186d2e83d4b61027.htm" target="_blank" rel="noopener"><!--[mako (he)]-->22</a>,
      <a href="https://youtu.be/p1LgrykDzy0?t=1174" target="_blank" rel="noopener"><!--[kan (he)]-->23</a>,
      <a href="https://www.futurity.org/edit-video-algorithm-2082552/" target="_blank" rel="noopener"><!--[Futurity]-->24</a>,
      <a href="https://www.zerohedge.com/news/2019-06-13/watch-scientists-create-deepfake-software-allowing-anyone-edit-anything-anyone-says" target="_blank" rel="noopener"><!--[ZeroHedge]-->25</a>,
      <a href="https://www.sciencealert.com/deepfake-ai-algorithms-can-now-take-text-and-turn-it-into-words-spoken-in-a-video" target="_blank" rel="noopener"><!--[ScienceAlert]-->26</a>,
      <a href="https://musically.com/2019/06/10/researchers-reveal-tech-to-modify-talking-head-videos-by-editing-text/" target="_blank" rel="noopener"><!--[musically]-->27</a>,
      <a href="https://newatlas.com/edit-talking-head-text-deepfake/60160/" target="_blank" rel="noopener"><!--[New Atlas]-->28</a>,
      <a href="https://www.eyerys.com/articles/news/ai-can-change-what-people-say-video-just-editing-its-transcript" target="_blank" rel="noopener"><!--[eyerys]-->29</a>,
      <a href="http://www.technovelgy.com/ct/Science-Fiction-News.asp?NewsNum=5638" target="_blank" rel="noopener"><!--[technovelgy]-->30</a>,
      <a href="https://www.innovationtoronto.com/2019/06/synthetic-content-algorithm-makes-editing-video-as-easy-as-editing-text-2/" target="_blank" rel="noopener"><!--[Innovation Toronto]-->31</a>,
      <a href="https://www.techeblog.com/text-based-editing-talking-head-ai-words-mouths/" target="_blank" rel="noopener"><!--[techeblog]-->32</a>].
    </div>

    <br>

    Another interesting read that mentions our research:
    <a href="https://www.washingtonpost.com/technology/2019/06/12/top-ai-researchers-race-detect-deepfake-videos-we-are-outgunned/?utm_term=.3e4a667ec626" target="_blank" rel="noopener">[Washington Post]</a>.
    <br><br>

    Wired video on photography, referencing some of our past research:
    <a href="https://www.youtube.com/watch?v=OrPkTs-i75A" target="_blank" rel="noopener">[video]</a>.
    <br><br>

    Our research letter in JAMA Facial Plastic Surgery
    <!--(The Journal of the American Medical Association)-->
    received some media attention!
    As of April 6, 2018, it has the #1 Altmetric score in JAMA Facial Plastic Surgery (#1 of 566) and is in the top 0.02% of all outputs tracked by Altmetric (#2219 of 9,569,234).
    <a href="https://www.altmetric.com/details/33822746#score" target="_blank" rel="noopener">[Altmetric]</a>
    <!--
    <script type="text/javascript" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
    <div class="altmetric-embed" data-badge-type="donut" data-altmetric-id="33822746" />
    -->
    <br><br>

    Nasal Distortion in Short-Distance Photographs: The Selfie Effect (2018) <br>
    <a href="https://www.cnn.com/2018/03/01/health/selfies-make-noses-look-bigger-study/index.html" target="_blank" rel="noopener">[CNN]</a>
    <a href="https://www.washingtonpost.com/news/morning-mix/wp/2018/03/07/hate-your-selfie-theres-a-good-reason-says-new-study/" target="_blank" rel="noopener">[The Washington Post]</a>
    <a href="https://www.cbsnews.com/news/selfies-distort-faces-30-percent-like-funhouse-mirror-study-finds/" target="_blank" rel="noopener">[CBS News]</a>
    <a href="https://www.forbes.com/sites/brucelee/2018/03/02/in-a-selfie-here-is-how-much-bigger-your-nose-appears/" target="_blank" rel="noopener">[Forbes]</a>
    <a href="https://www.wired.com/story/use-science-not-surgery-to-create-your-best-selfie" target="_blank" rel="noopener">[Wired]</a>
    <a href="https://www.theverge.com/2018/3/1/17067176/selfies-nose-jobs-plastic-surgery-rhinoplasty-wide-angle-distortion" target="_blank" rel="noopener">[The Verge]</a>
    <a href="http://abc13.com/society/selfies-can-make-your-nose-look-huge-researchers-say/3162400/" target="_blank" rel="noopener">[ABC]</a>
    <a href="http://www.newsweek.com/more-people-want-nose-jobs-because-selfies-make-theirs-look-30-bigger-doctors-828035" target="_blank" rel="noopener">[Newsweek]</a>
    <a href="https://www.vox.com/2018/3/26/17152780/selfie-nose-distortion" target="_blank" rel="noopener">[Vox]</a>
    <a href="http://www.dailymail.co.uk/health/article-5450245/Selfies-make-snouts-look-BIGGER-drive-people-NOSE-JOBS.html" target="_blank" rel="noopener">[Daily Mail]</a>
    <a href="https://www.cnet.com/news/more-plastic-surgery-as-selfies-make-nose-bigger-rutgers-study/" target="_blank" rel="noopener">[cnet]</a>
    and many more:

    <div>
      [<a href="http://billingsgazette.com/lifestyles/do-selfies-make-your-nose-look-big-rutgers-study-says/article_4d9ab349-6ba5-5cf9-ad82-ba24330aea0c.html" target="_blank" rel="noopener">1</a>,
      <a href="http://billingsgazette.com/lifestyles/simplemost/selfies-make-your-nose-look-bigger-study-says/article_75fa5cdb-417e-5d5f-979f-60187fc567a6.html" target="_blank" rel="noopener">2</a>,
      <a href="http://denver.cbslocal.com/2018/03/01/selfies-nose-job-plastic-surgery-doctors/" target="_blank" rel="noopener">3</a>,
      <a href="http://detroit.cbslocal.com/2018/03/01/selfies-nose-job-plastic-surgery-doctors/" target="_blank" rel="noopener">4</a>,
      <a href="http://emais.estadao.com.br/noticias/comportamento,selfies-podem-fazer-o-nariz-parecer-30-mais-largo-aponta-estudo,70002212443" target="_blank" rel="noopener">5</a>,
      <a href="http://gooddaysacramento.cbslocal.com/2018/03/01/selfies-nose-job-plastic-surgery-doctors/" target="_blank" rel="noopener">6</a>,
      <a href="http://healthmedicinet.com/news/distortive-effects-of-short-distance-photographs-on-nasal-appearance-the-selfie-effect/" target="_blank" rel="noopener">7</a>,
      <a href="http://home.bt.com/news/science-news/selfies-make-your-nose-look-30-bigger-says-plastic-surgeon-11364255158336" target="_blank" rel="noopener">8</a>,
      <a href="http://home.bt.com/tech-gadgets/tech-news/selfies-make-your-nose-look-bigger-plastic-surgeon-11364255158336" target="_blank" rel="noopener">9</a>,
      <a href="http://host.madison.com/lifestyles/do-selfies-make-your-nose-look-big-rutgers-study-says/article_714fc425-0b5a-5816-9b76-56d968556c65.html" target="_blank" rel="noopener">10</a>,
      <a href="http://journalstar.com/lifestyles/do-selfies-make-your-nose-look-big-rutgers-study-says/article_a2878b4b-fbe0-57d8-a619-d328f7cf5307.html" target="_blank" rel="noopener">11</a>,
      <a href="http://nationalpost.com/news/selfies-make-noses-look-30-per-cent-bigger-but-thats-no-reason-for-plastic-surgery" target="_blank" rel="noopener">12</a>,
      <a href="http://q13fox.com/2018/03/01/selfies-make-your-nose-look-30-bigger-study-says/" target="_blank" rel="noopener">13</a>,
      <a href="http://sevilla.abc.es/ciencia/abci-selfies-hacen-nariz-parezca-mas-grande-201803052045_noticia.html" target="_blank" rel="noopener">14</a>,
      <a href="http://triblive.com/usworld/world/13370470-74/do-selfies-make-your-nose-look-big-study-says-yes" target="_blank" rel="noopener">15</a>,
      <a href="http://wivb.com/2018/03/01/selfies-distort-faces-like-a-funhouse-mirror-study-finds/" target="_blank" rel="noopener">16</a>,
      <a href="http://wtnh.com/2018/03/01/selfies-make-your-nose-look-30-bigger-study-says/" target="_blank" rel="noopener">17</a>,
      <a href="http://www.20min.ch/digital/news/story/Auf-Selfies-wirkt-die-Nase-bis-zu-30-Prozent-groesser-27627022" target="_blank" rel="noopener">18</a>,
      <a href="http://www.abc.es/ciencia/abci-selfies-hacen-nariz-parezca-mas-grande-201803052045_noticia.html" target="_blank" rel="noopener">19</a>,
      <a href="http://www.actionnewsnow.com/content/national/475553433.html" target="_blank" rel="noopener">20</a>,
      <a href="http://www.arkansasonline.com/news/2018/mar/27/selfies-only-the-nose-knows-20180327/" target="_blank" rel="noopener">21</a>,
      <a href="http://www.azfamily.com/story/37623964/selfies-make-your-nose-look-30-bigger-study-says" target="_blank" rel="noopener">22</a>,
      <a href="http://www.bostonherald.com/lifestyle/style_fashion/2018/03/do_selfies_make_your_nose_look_big_rutgers_study_says_yes" target="_blank" rel="noopener">23</a>,
      <a href="http://www.bullfax.com/?q=node-selfies-make-noses-look-30-cent-bigger-%E2%80%94-that%E2%80%99s-no-reas" target="_blank" rel="noopener">24</a>,
      <a href="http://www.businessinsider.com/how-to-look-good-in-pictures-nose-looks-bigger-in-selfies-2018-3" target="_blank" rel="noopener">25</a>,
      <a href="http://www.businessinsider.com/r-selfies-distort-the-face-plastic-surgeons-warn-2018-3" target="_blank" rel="noopener">26</a>,
      <a href="http://www.businessinsider.my/how-to-look-good-in-pictures-nose-looks-bigger-in-selfies-2018-3/" target="_blank" rel="noopener">27</a>,
      <a href="http://www.businessinsider.sg/how-to-look-good-in-pictures-nose-looks-bigger-in-selfies-2018-3/" target="_blank" rel="noopener">28</a>,
      <a href="http://www.cbc.ca/news/health/selfies-distort-nose-1.4566113" target="_blank" rel="noopener">29</a>,
      <a href="http://www.columbian.com/news/2018/mar/08/study-finds-selfies-arent-putting-best-face-forward/" target="_blank" rel="noopener">30</a>,
      <a href="http://www.erienewsnow.com/story/37623964/selfies-make-your-nose-look-30-bigger-study-says" target="_blank" rel="noopener">31</a>,
      <a href="http://www.futurity.org/faces-selfies-self-image-1694162/" target="_blank" rel="noopener">32</a>,
      <a href="http://www.heraldtribune.com/zz/shareable/20180306/do-selfies-make-your-nose-look-big-rutgers-study-says-yes" target="_blank" rel="noopener">33</a>,
      <a href="http://www.ibtimes.com.au/taking-selfies-12-inches-away-increases-perceived-nose-size-report-1565614" target="_blank" rel="noopener">34</a>,
      <a href="http://www.journalnow.com/news/trending/hate-your-selfie-there-may-be-a-good-reason-a/article_2db4081f-c62e-5e0e-8c55-1a718a60ae2b.html" target="_blank" rel="noopener">35</a>,
      <a href="http://www.krone.at/1660590" target="_blank" rel="noopener">36</a>,
      <a href="http://www.kxxv.com/story/37637039/medical-poll-finds-selfies-contributing-to-plastic-surgery-increase" target="_blank" rel="noopener">37</a>,
      <a href="http://www.lavozdigital.es/ciencia/abci-selfies-hacen-nariz-parezca-mas-grande-201803052045_noticia.html" target="_blank" rel="noopener">38</a>,
      <a href="http://www.liberation.fr/sciences/2018/03/09/halte-aux-rhinoplasties-inutiles-les-selfies-grossissent-le-nez-c-est-prouve_1634888" target="_blank" rel="noopener">39</a>,
      <a href="http://www.live5news.com/story/37637039/medical-poll-finds-selfies-contributing-to-plastic-surgery-increase" target="_blank" rel="noopener">40</a>,
      <a href="http://www.medindia.net/news/selfies-make-your-nose-appear-bigger-177537-1.htm" target="_blank" rel="noopener">41</a>,
      <a href="http://www.meteoweb.eu/2018/03/bellezza-ritocchi-naso-selfie-studio-dimostra-effetto-distorsivo/1055959/" target="_blank" rel="noopener">42</a>,
      <a href="http://www.modernhealthcare.com/article/20180317/NEWS/180319912" target="_blank" rel="noopener">43</a>,
      <a href="http://www.newshub.co.nz/home/health/2018/03/bad-selfies-blamed-for-rise-in-unnecessary-plastic-surgery.html" target="_blank" rel="noopener">44</a>,
      <a href="http://www.newswise.com/articles/view/690357/" target="_blank" rel="noopener">45</a>,
      <a href="http://www.nwitimes.com/lifestyles/do-selfies-make-your-nose-look-big-rutgers-study-says/article_f29defe4-b504-52ae-8187-576409618c2f.html" target="_blank" rel="noopener">46</a>,
      <a href="http://www.nydailynews.com/life-style/selfie-effect-nose-bigger-article-1.3849458" target="_blank" rel="noopener">47</a>,
      <a href="http://www.nzherald.co.nz/lifestyle/news/article.cfm?c_id=6&objectid=12004864" target="_blank" rel="noopener">48</a>,
      <a href="http://www.philly.com/philly/health/selfie-make-nose-look-bigger-rutgers-study-20180301.html" target="_blank" rel="noopener">49</a>,
      <a href="http://www.philly.com/philly/health/topics/HealthDay731592_20180301_Think_Your_Nose_Is_Too_Big__Selfies_Might_Be_to_Blame.html" target="_blank" rel="noopener">50</a>,
      <a href="http://www.phillyvoice.com/heres-why-your-nose-looks-big-those-selfies/" target="_blank" rel="noopener">51</a>,
      <a href="http://www.physiciansbriefing.com/Article.asp?AID=731642" target="_blank" rel="noopener">52</a>,
      <a href="http://www.pressofatlanticcity.com/news/trending/hate-your-selfie-there-may-be-a-good-reason-a/article_dc5acecc-96cd-5e9e-8b21-7675b714ce10.html" target="_blank" rel="noopener">53</a>,
      <a href="http://www.richmond.com/news/trending/hate-your-selfie-there-may-be-a-good-reason-a/article_a3ea5915-205e-5b67-8e98-662d582f8f66.html" target="_blank" rel="noopener">54</a>,
      <a href="http://www.rocketnews.com/2018/02/think-your-nose-is-too-big-selfies-might-be-to-blame/" target="_blank" rel="noopener">55</a>,
      <a href="http://www.sciencenewsline.com/news/2018030217470062.html" target="_blank" rel="noopener">56</a>,
      <a href="http://www.scmp.com/news/world/article/2135408/did-selfie-make-your-nose-look-big-science-says-yes-so-theres-probably-no" target="_blank" rel="noopener">57</a>,
      <a href="http://www.sj-r.com/zz/shareable/20180306/do-selfies-make-your-nose-look-big-rutgers-study-says-yes" target="_blank" rel="noopener">58</a>,
      <a href="http://www.southcoasttoday.com/zz/shareable/20180306/do-selfies-make-your-nose-look-big-rutgers-study-says-yes" target="_blank" rel="noopener">59</a>,
      <a href="http://www.spektrum.de/news/mehr-schoenheits-ops-wegen-social-media/1548631" target="_blank" rel="noopener">60</a>,
      <a href="http://www.techtimes.com/articles/222179/20180303/selfies-can-make-your-nose-look-30-percent-bigger.htm" target="_blank" rel="noopener">61</a>,
      <a href="http://www.techtimes.com/articles/222250/20180305/selfies-making-more-people-head-plastic-surgeon-nose-jobs.htm" target="_blank" rel="noopener">62</a>,
      <a href="http://www.valuewalk.com/2018/03/selfie-effect-nose-job/" target="_blank" rel="noopener">63</a>,
      <a href="http://www.watertowndailytimes.com/national/do-selfies-make-your-nose-look-bignew-study-shows-that-they-do--20180305" target="_blank" rel="noopener">64</a>,
      <a href="http://www.winonadailynews.com/lifestyles/do-selfies-make-your-nose-look-big-rutgers-study-says/article_b2469ae4-0556-5e2e-a22c-bdf29b122436.html" target="_blank" rel="noopener">65</a>,
      <a href="http://www.wmur.com/article/how-to-take-good-selfies-science/19177961" target="_blank" rel="noopener">66</a>,
      <a href="https://au.news.yahoo.com/a/39436524/selfies-distort-the-face-plastic-surgeons-warn/" target="_blank" rel="noopener">67</a>,
      <a href="https://ca.news.yahoo.com/selfies-nose-look-bigger-third-154557928.html" target="_blank" rel="noopener">68</a>,
      <a href="https://consumer.healthday.com/cosmetic-information-8/misc-looks-health-news-449/think-your-nose-is-too-big-selfies-might-be-to-blame-731592.html" target="_blank" rel="noopener">69</a>,
      <a href="https://donna.fanpage.it/insoddisfatto-dei-selfie-e-colpa-del-naso/" target="_blank" rel="noopener">70</a>,
      <a href="https://factsherald.com/your-nose-appears-30-bigger-in-a-selfie-study-reveals/" target="_blank" rel="noopener">71</a>,
      <a href="https://fr.news.yahoo.com/halte-rhinoplasties-inutiles-selfies-grossissent-nez-c-prouv%C3%A9-090335861.html" target="_blank" rel="noopener">72</a>,
      <a href="https://health.usnews.com/health-care/articles/2018-03-01/think-your-nose-is-too-big-selfies-might-be-to-blame" target="_blank" rel="noopener">73</a>,
      <a href="https://patch.com/new-jersey/newbrunswick/selfies-are-making-your-nose-look-bigger-rutgers-study-finds" target="_blank" rel="noopener">74</a>,
      <a href="https://phys.org/news/2018-03-distortive-effects-short-distance-nasal.html" target="_blank" rel="noopener">75</a>,
      <a href="https://uk.finance.yahoo.com/news/selfies-nose-look-bigger-third-154557928.html" target="_blank" rel="noopener">76</a>,
      <a href="https://uk.news.yahoo.com/selfies-nose-look-30-bigger-192932124.html" target="_blank" rel="noopener">77</a>,
      <a href="https://wtop.com/tech/2018/03/plastic-surgeons-want-know-selfies/" target="_blank" rel="noopener">78</a>,
      <a href="https://www.aafprs.org/media/stats_polls/m_stats.html" target="_blank" rel="noopener">79</a>,
      <a href="https://www.abcnyheter.no/helse-og-livsstil/livet/2018/03/07/195377920/selfien-gir-ikke-et-riktig-bilde-av-deg" target="_blank" rel="noopener">80</a>,
      <a href="https://www.ajc.com/news/world/selfies-can-make-your-nose-look-percent-bigger-study-says/0Yg78YfQkdVFuznxj3yURN/" target="_blank" rel="noopener">81</a>,
      <a href="https://www.arcamax.com/currentnews/newsheadlines/s-2055175" target="_blank" rel="noopener">82</a>,
      <a href="https://www.breakingnews.ie/world/selfies-make-your-nose-look-30-bigger-says-plastic-surgeon-830488.html" target="_blank" rel="noopener">83</a>,
      <a href="https://www.businessinsider.in/Why-your-nose-looks-so-big-when-you-take-a-selfie-and-how-to-fix-it/articleshow/63190961.cms" target="_blank" rel="noopener">84</a>,
      <a href="https://www.bustle.com/p/selfies-may-be-influencing-people-to-get-more-plastic-surgery-according-to-a-new-study-heres-what-you-need-to-know-8392906" target="_blank" rel="noopener">85</a>,
      <a href="https://www.cbosnews.com/news/selfies-distort-faces-30-percent-like-funhouse-mirror-study-finds/" target="_blank" rel="noopener">86</a>,
      <a href="https://www.centralmaine.com/2018/03/01/is-that-really-you%E2%80%A8selfies-can-make%E2%80%A8noses-look-bigger%E2%80%A8researchers-find/" target="_blank" rel="noopener">87</a>,
      <a href="https://www.cnet.com/es/noticias/selfies-hacen-que-te-veas-mas-narigon/" target="_blank" rel="noopener">88</a>,
      <a href="https://www.cosmopolitan.com/uk/body/a19051030/selfie-distort-face-shape-nose-study/" target="_blank" rel="noopener">89</a>,
      <a href="https://www.dailystar.co.uk/news/latest-news/685734/UK-nose-job-operation-selfies-cosmetic-surgeons-Kylie-Jenner-Jennifer-Aniston" target="_blank" rel="noopener">90</a>,
      <a href="https://www.doctorslounge.com/index.php/news/hd/78805" target="_blank" rel="noopener">91</a>,
      <a href="https://www.drugs.com/news/think-your-nose-big-selfies-might-blame-68867.html" target="_blank" rel="noopener">92</a>,
      <a href="https://www.eurekalert.org/pub_releases/2018-03/ru-deo030118.php" target="_blank" rel="noopener">93</a>,
      <a href="https://www.eveningexpress.co.uk/news/selfies-make-your-nose-look-30-bigger-says-plastic-surgeon/" target="_blank" rel="noopener">94</a>,
      <a href="https://www.eveningtelegraph.co.uk/2018/03/01/selfies-make-your-nose-look-30-bigger-says-plastic-surgeon/" target="_blank" rel="noopener">95</a>,
      <a href="https://www.express.co.uk/life-style/health/926138/nose-jobs-selfies-surgeons-twitter-facebook-camera-face-distortion" target="_blank" rel="noopener">96</a>,
      <a href="https://www.expressen.se/halsoliv/skonhet-1/forskare-varnar-selfies-far-din-nasa-att-se-storre-ut/" target="_blank" rel="noopener">97</a>,
      <a href="https://www.focus.it/cultura/curiosita/qual-e-la-distanza-giusta-per-un-selfie" target="_blank" rel="noopener">98</a>,
      <a href="https://www.futura-sciences.com/sciences/actualites/physique-science-decalee-selfies-grossissent-nez-70504/" target="_blank" rel="noopener">99</a>,
      <a href="https://www.guelphmercury.com/living-story/8314111-do-selfies-make-your-nose-look-big-rutgers-study-says-yes/" target="_blank" rel="noopener">100</a>,
      <a href="https://www.guelphmercury.com/news-story/8314715-hate-your-selfie-there-may-be-a-good-reason-a-new-study-says/" target="_blank" rel="noopener">101</a>,
      <a href="https://www.healthline.com/health-news/do-you-know-somebody-who-suffers-from-selfitis" target="_blank" rel="noopener">102</a>,
      <a href="https://www.ksl.com/?sid=46271516&nid=1010" target="_blank" rel="noopener">103</a>,
      <a href="https://www.livescience.com/61896-why-selfies-distort-your-face-math.html" target="_blank" rel="noopener">104</a>,
      <a href="https://www.longroom.com/discussion/913373/selfies-distort-your-face-by-30-and-heres-the-math-to-back-it-up" target="_blank" rel="noopener">105</a>,
      <a href="https://www.longroom.com/discussion/914265/yes-selfies-do-make-your-nose-look-bigger" target="_blank" rel="noopener">106</a>,
      <a href="https://www.medicinenet.com/script/main/art.asp?articlekey=210540" target="_blank" rel="noopener">107</a>,
      <a href="https://www.menaiset.fi/artikkeli/hyva-olo/iso-nenasi-taydellinen-juuri-noin-sivuprofiiliselfiet-levittavat-vihdoin-myos" target="_blank" rel="noopener">108</a>,
      <a href="https://www.metro.us/body-and-mind/health/selfies-distort-face" target="_blank" rel="noopener">109</a>,
      <a href="https://www.msn.com/en-ca/lifestyle/whats-hot/hate-your-selfie-there-may-be-a-good-reason-says-new-study/ar-BBJYv6P" target="_blank" rel="noopener">110</a>,
      <a href="https://www.msn.com/en-gb/lifestyle/style/this-is-the-real-reason-why-you-hate-your-face-in-selfies/ar-BBJV7aw" target="_blank" rel="noopener">111</a>,
      <a href="https://www.msn.com/en-us/health/wellness/the-selfie-effect-selfies-make-noses-appear-larger-leading-to-more-nose-job-requests/ar-BBJL7qM" target="_blank" rel="noopener">112</a>,
      <a href="https://www.msn.com/en-us/news/technology/yes-selfies-do-make-your-nose-look-bigger/ar-BBJMWhM" target="_blank" rel="noopener">113</a>,
      <a href="https://www.msn.com/es-co/noticias/tecnologia/la-ciencia-explica-por-qu%C3%A9-tu-nariz-parece-gigante-en-tus-selfies/ar-BBJYTs8" target="_blank" rel="noopener">114</a>,
      <a href="https://www.newbeauty.com/blog/dailybeauty/12001-selfies-distort-your-face/" target="_blank" rel="noopener">115</a>,
      <a href="https://www.news-medical.net/news/20180302/Selfie-nose-does-look-bigger-finds-study.aspx" target="_blank" rel="noopener">116</a>,
      <a href="https://www.newsmax.com/health/health-news/selfies-plastic-surgery-distort/2018/03/07/id/847390/" target="_blank" rel="noopener">117</a>,
      <a href="https://www.pharmazeutische-zeitung.de/index.php?id=74719" target="_blank" rel="noopener">118</a>,
      <a href="https://www.pressherald.com/2018/03/01/is-that-really-you%E2%80%A8selfies-can-make%E2%80%A8noses-look-bigger%E2%80%A8researchers-find/" target="_blank" rel="noopener">119</a>,
      <a href="https://www.refinery29.com/2018/03/192353/selfies-facial-distortion-study" target="_blank" rel="noopener">120</a>,
      <a href="https://www.sciencedaily.com/releases/2018/03/180301115349.htm" target="_blank" rel="noopener">121</a>,
      <a href="https://www.smithsonianmag.com/smart-news/your-selfies-are-lying-you-180968340/" target="_blank" rel="noopener">122</a>,
      <a href="https://www.stuff.co.nz/technology/digital-living/101971350/Do-selfies-make-your-nose-look-big-Study-says-yes" target="_blank" rel="noopener">123</a>,
      <a href="https://www.telegraph.co.uk/technology/2018/03/02/selfies-make-nose-look-bigger-third-compared-real-life-research/" target="_blank" rel="noopener">124</a>,
      <a href="https://www.thespec.com/living-story/8314111-do-selfies-make-your-nose-look-big-rutgers-study-says-yes/" target="_blank" rel="noopener">125</a>,
      <a href="https://www.thespec.com/news-story/8314715-hate-your-selfie-there-may-be-a-good-reason-a-new-study-says/" target="_blank" rel="noopener">126</a>,
      <a href="https://www.thestar.com.my/tech/tech-news/2018/03/05/do-selfies-make-your-nose-look-big-study-says-yes/" target="_blank" rel="noopener">127</a>,
      <a href="https://www.thestar.com/business/2018/03/09/hate-your-selfie-dont-rush-out-and-get-a-nose-job.html" target="_blank" rel="noopener">128</a>,
      <a href="https://www.thetimes.co.uk/article/selfie-distortion-is-making-patients-seek-surgery-tc23z5bgx" target="_blank" rel="noopener">129</a>,
      <a href="https://www.timeslive.co.za/sunday-times/lifestyle/fashion-and-beauty/2018-03-02-does-my-nose-look-big-in-this-selfie-yes/" target="_blank" rel="noopener">130</a>,
      <a href="https://www.usnews.com/news/national-news/articles/2018-03-01/the-selfie-effect-selfies-make-noses-appear-larger-leading-to-more-nose-job-requests" target="_blank" rel="noopener">131</a>,
      <a href="https://www.vox.com/science-and-health/2018/3/1/17059566/plastic-surgery-selfie-distortion" target="_blank" rel="noopener">132</a>,
      <a href="https://www.washingtontimes.com/news/2018/mar/1/smartphone-vanity-selfies-actually-make-your-nose-/" target="_blank" rel="noopener">133</a>,
      <a href="https://www.wltz.com/2018/03/01/study-selfies-make-nose-look-30-bigger/" target="_blank" rel="noopener">134</a>,
      <a href="https://www.womenshealthmag.com/life/a19040905/selfie-nose-study/" target="_blank" rel="noopener">135</a>,
      <a href="https://www.yahoo.com/lifestyle/actually-scientific-reason-why-selfies-223101638.html" target="_blank" rel="noopener">136</a>,
      <a href="https://www.ynet.co.il/articles/0,7340,L-5144350,00.html" target="_blank" rel="noopener">137</a>,
      <a href="https://www.zmescience.com/science/news-science/selfies-make-hate-nose-02032018/" target="_blank" rel="noopener">138</a>].
    </div>

    <br>

    Face Warp, Art of Science (2017) <br>
    <a href="https://paw.princeton.edu/article/science-art-0" target="_blank" rel="noopener">[PAW]</a>

    <br><br>

    Radio story on the future of media manipulation (2017, in German) <br>
    <a href="http://oe1.orf.at/programm/20170512/474372" target="_blank" rel="noopener">[webpage]</a>
    <a href="./media/Matrix_Manipulierte_Welt.mp3" target="_blank" rel="noopener">[mp3]</a>

    <br><br>

    Perspective-aware Manipulation of Portrait Photos (2016) <br>
    <a href="http://gizmodo.com/this-photo-editing-software-hopes-to-make-your-selfies-1784562334" target="_blank" rel="noopener">[Gizmodo]</a>
    <a href="http://motherboard.vice.com/read/programmers-made-a-selfie-filter-to-make-you-like-more-like-you-sorry" target="_blank" rel="noopener">[Motherboard]</a>
    <a href="http://www.lonelyplanet.com/news/2016/08/01/new-selfie-software/" target="_blank" rel="noopener">[Lonely&nbsp;Planet]</a>
    <a href="http://www.forbes.com/sites/paulmonckton/2016/07/31/digital-selfie-correction/#44d24a97773b" target="_blank" rel="noopener">[Forbes]</a>
    <a href="http://money.cnn.com/video/technology/2016/08/10/selfie-editing-tool.cnnmoney/" target="_blank" rel="noopener">[CNN]</a>
    <a href="http://www.fastcodesign.com/3062482/evidence/how-to-get-rid-of-your-big-nose-in-selfies" target="_blank" rel="noopener">[FastCompany]</a>
    <a href="http://cacm.acm.org/news/205358-selfie-righteous-new-tool-corrects-angles-and-distances-in-portraits/fulltext" target="_blank" rel="noopener">[CACM]</a>
    <a href="https://www.dpreview.com/news/9715966999/new-technology-alters-perspective-in-selfies-generates-3d-images-and-more" target="_blank" rel="noopener">[DPReview]</a>
    <a href="http://www.dailymail.co.uk/sciencetech/article-3713494/Want-boost-selfie-game-Princeton-app-claims-able-make-snaps-look-like-professional-photos.html" target="_blank" rel="noopener">[DailyMail]</a>
    <a href="https://www.yahoo.com/tech/yes-selfies-nose-look-big-232520832.html" target="_blank" rel="noopener">[Yahoo&nbsp;Tech]</a>
    <a href="http://www.digitaltrends.com/photography/perspective-aware-manipulation-research/" target="_blank" rel="noopener">[Digital&nbsp;Trends]</a>
    <a href="http://newatlas.com/princeton-better-selfies-tool/44651/" target="_blank" rel="noopener">[Gizmag]</a>
    <a href="https://www.wired.de/collection/tech/dieser-filter-laesst-eure-selfies-realer-wirken" target="_blank" rel="noopener">[Wired&nbsp;DE]</a>
    <a href="http://www.spiegel.de/netzwelt/web/bildbearbeitung-mit-3d-modellen-zum-perfekten-selfie-a-1105696.html" target="_blank" rel="noopener">[Spiegel]</a>
    <a href="http://petapixel.com/2016/07/29/algorithm-warps-wide-angle-selfies-look-like-normal-portraits/" target="_blank" rel="noopener">[PetaPixel]</a>
    <a href="http://timesofindia.indiatimes.com/tech/tech-news/Princeton-University-researchers-develop-tool-that-helps-you-take-better-selfies/articleshow/53449887.cms" target="_blank" rel="noopener">[TOI]</a>
    <a href="http://www.science20.com/news_articles/selfie_righteous_new_tool_corrects_angles_and_distances_in_portraits-177402" target="_blank" rel="noopener">[Science&nbsp;2.0]</a>
    <a href="http://www.cnet.com/news/no-more-selfie-distortion-software-aims-to-fix-your-unflattering-face/" target="_blank" rel="noopener">[cnet]</a>
    <a href="http://www.nj.com/education/2016/07/a_better_selfie_princeton_researchers_may_have_just_fixed_face_photos.html" target="_blank" rel="noopener">[NJ]</a>
    <a href="http://www.diyphotography.net/software-makes-selfiels-look-taken-200mm-lens/" target="_blank" rel="noopener">[DIYPhotography]</a>
    <a href="http://www.designboom.com/technology/selfie-corrector-software-more-attractive-princeton-university-research-08-01-2016/" target="_blank" rel="noopener">[designboom]</a>
    <a href="https://fstoppers.com/science/princeton-research-makes-selfies-look-they-were-shot-portrait-lenses-141352" target="_blank" rel="noopener">[Fstoppers]</a>
    <a href="https://www.inverse.com/article/19934-princeton-computer-science-selfie-instagram-photo-manipulation" target="_blank" rel="noopener">[Inverse]</a>
    <a href="http://www.princeton.edu/engineering/news/archive/?id=16974" target="_blank" rel="noopener">[Princeton]</a>
    <a href="http://www.icsjournal.org/articles/no-more-photobombs" target="_blank" rel="noopener">[ICSJ]</a>
    <a href="https://www.youtube.com/watch?v=UjuBLS15JqM" target="_blank" rel="noopener">[Two Minute Papers]</a>

    <br><br>

    Finding Distractors In Images (2015) <br>
    <a href="http://www.popsci.com/software-automatically-takes-out-distracting-parts-your-photos" target="_blank" rel="noopener">[Popular Science]</a>
    <a href="http://www.msn.com/en-au/money/other/software-automatically-takes-out-distracting-items-from-your-photos/ar-BBmbgdM" target="_blank" rel="noopener">[MSN]</a>
    <a href="http://www.dailymail.co.uk/sciencetech/article-3266933/Delete-ex-photobomber-single-click-Adobe-shows-software-remove-people-cars-snaps.html" target="_blank" rel="noopener">[Daily Mail]</a>
    <a href="http://www.princeton.edu/main/news/archive/S44/06/19O11/" target="_blank" rel="noopener">[Princeton]</a>
    <a href="http://www.ibtimes.co.uk/adobe-princeton-develop-software-automatically-remove-distracting-photobombs-images-1517532" target="_blank" rel="noopener">[IBT]</a>
    <a href="https://uk.news.yahoo.com/adobe-princeton-develop-software-automatically-155320284.html" target="_blank" rel="noopener">[Yahoo]</a>
    <a href="https://www.ephotozine.com/article/new-technology-automatically-detects-and-removes-distracting-objects-from-images-28064" target="_blank" rel="noopener">[ephotozine]</a> <br>
    <a href="https://www.youtube.com/watch?v=UeQgmWuQMTU" target="_blank" rel="noopener">[Adobe MAX (video)]</a>
    <a href="http://thenextweb.com/creativity/2015/10/07/adobe-sneak-peeks-target-photobombs-3d/" target="_blank" rel="noopener">[TNW]</a>
    <a href="https://www.inverse.com/article/6085-princeton-computer-scientists-want-to-give-you-spellcheck-for-your-photos" target="_blank" rel="noopener">[Inverse]</a>
    <a href="https://paw.princeton.edu/issues/2015/12/02/pages/4712/index.xml" target="_blank" rel="noopener">[PAW]</a>

    <br><br>

    Exposed, Art of Science (2013) <br>
    <a href="http://www.wired.com/2013/06/art-of-science/" target="_blank" rel="noopener">[Wired]</a>
    <a href="http://www.popsci.com/science/article/2013-05/8-gorgeous-science-images?image=3" target="_blank" rel="noopener">[Popular Science]</a>
    <a href="https://www.princeton.edu/artofscience/gallery2013/one.php%3Fid=297.html" target="_blank" rel="noopener">[Princeton]</a>
    <a href="http://news.nationalgeographic.com/news/2013/05/pictures/130529-best-science-pictures-mouse-embryo-worms-research/" target="_blank" rel="noopener">[National Geographic]</a>
    <a href="http://gizmodo.com/10-of-the-years-most-beautiful-science-images-508969751" target="_blank" rel="noopener">[Gizmodo]</a>
    <a href="http://www.livescience.com/34494-images-reveal-beauty-in-science.html" target="_blank" rel="noopener">[LiveScience]</a>
    <a href="http://www.maxisciences.com/science/ohad-fried-a-reconstruit-ce-visage-a-partir-d-images-videos-floutees-a-la-base-pour-preserver-l-anonymat_pic59780.html" target="_blank" rel="noopener">[Maxisciences]</a>
    <br>
  </div>

  <div class="clear"></div>

  <h2 id="awards">Grants, Awards & Honors</h2>
  <div>
    <span class=fixed>[RUni ]</span> Outstanding faculty researcher, CS department, 2022.<br>
    <span class=fixed>[RUni ]</span> Israel Science Foundation, personal research grant (4 years), 2021.<br>
    <span class=fixed>[RUni ]</span> Industry grant, Lightricks Ltd.<br>
    <span class=fixed>[Post ]</span> <a href="https://brown.columbia.edu/people/" target="_blank" rel="noopener">The Brown Institute for Media Innovation, Fellow 2017-2019</a> <br>
    <span class=fixed>[Post ]</span> <a href="https://news.stanford.edu/2018/05/08/brown-institute-stanford-columbia-universities-announces-magic-grant-winners/" target="_blank" rel="noopener">The Brown Institute for Media Innovation, Magic Grant 2018-2019</a> <br>
    <span class=fixed>[Post ]</span> <a href="https://brown.columbia.edu/portfolio/camera-observa/" target="_blank" rel="noopener">The Brown Institute for Media Innovation, Magic Grant 2017-2018</a> <br>
    <span class=fixed>[Ph.D.]</span> <a href="http://www.siebelscholars.com/news/siebel-scholars-foundation-announces-2017-siebel-scholars" title="awarded annually for academic excellence and demonstrated leadership to over 90 top students from the world's leading graduate schools" target="_blank" rel="noopener">Siebel Scholar, Class of 2017</a> <br>
    <span class=fixed>[Ph.D.]</span>
    <a href="https://ai.google/research/outreach/phd-fellowship/recipients/?category=2014" target="_blank" rel="noopener">Google PhD Fellowship, 2014-2016</a> <br>
    <!--<span class=fixed>[Ph.D.]</span> <a href="http://giving.princeton.edu/scholarships-fellowships/fellowships/endowed2#w" target="_blank" rel="noopener">Gordon Y.S. Wu Fellowship in Engineering, 2012-2013</a> <br>-->
    <span class=fixed>[Ph.D.]</span> <a href="https://gradschool.princeton.edu/financial-support/fellowships/princeton-fellowships/gordon-wu-fellowship" target="_blank" rel="noopener">Gordon Y.S. Wu Fellowship in Engineering, 2012-2013</a> <br>
    <span class=fixed>[M.S. ]</span> Scholarship of excellence, School of Computer Science, The Hebrew University, 2010-2011 <br>
    <span class=fixed>[B.Sc.]</span> Graduated <em>magna cum laude</em> <br>
    <span class=fixed>[B.Sc.]</span> Amirim interdisciplinary honors program, 2007-2010 <br>
    <span class=fixed>[B.Sc.]</span> Scholarship of excellence, The Hebrew University, 2007-2010 <br>
    <span class=fixed>[B.Sc.]</span> Dean's list for 2007/8, 2008/9 and 2009/10 <br>
  </div>

  <div class="clear"></div>

  <h2 id="patents">Patents</h2>
  <div>
    Image Distractor Detection and Processing, <a href="https://www.google.com/patents/US9665962" target="_blank" rel="noopener">US9665962 B2</a> <br>
    <!--Ohad Fried, Eli Shechtman and Dan B. Goldman, 2017 -->
    Optimizing Photo Album Layouts, <a href="https://patents.google.com/patent/US9424671B1" target="_blank" rel="noopener">US9424671 B1</a>
    <!-- Stephen DiVerdi and Ohad Fried, 2016 -->
  </div>

  <div class="clear"></div>

  <h2 id="teaching">Teaching</h2>
  <div>
    Reichman University &#8212; "Advanced Programming", fall 2020, fall 2021, fall 2022, fall 2023 <br>
    Reichman University &#8212; "The Computer As A Content Creator", spring 2023 <br>
    Reichman University &#8212; "Advanced Seminar on Virtual Humans", spring 2022 <br>
    Reichman University &#8212; "Advanced Seminar on Neural Rendering", spring 2021 <br>
    Stanford University &#8212; <a href="https://magrawala.github.io/cs448v-sp19/" target="_blank" rel="noopener">"Computational Video Manipulation"</a>, spring 2019 <br>
    Princeton University &#8212; Teaching Assistant, "Computer Graphics", spring 2013 <br>
    Princeton University &#8212; Teaching Assistant, "Computer Vision", fall 2013 <br>
    Princeton University &#8212; Teaching Assistant, "MATLAB short course on programming", fall 2013 <br>
    Hebrew University of Jerusalem &#8212; "Introduction to Software and System Security", fall 2011/12 <br>
    Hebrew University of Jerusalem &#8212; "Introduction to Software and System Security", spring 2011
  </div>

  <!--
  <div class="clear"></div>
  <h2 id="work">Work</h2>
  <div>
    <b>Reichman University, 2020&#8211;Present.</b> Senior Lecturer (Assistant Professor). <br>
    <b>Deepdub.ai, 2021&#8211;2023.</b> Principal Research Scientist. <br>
    <b>Stanford University, 2017&#8211;2020.</b> Postdoctoral Research Scholar. <br>
    <b>Adobe, 2014, 2015.</b> Creative Technology Lab Intern (during PhD). <br>
    <b>Google, 2013.</b> Software Engineering Intern (during PhD). <br>
    <b>iStar Technologies, 2009&#8211;2010.</b> Head of Software Development. <br>
    <b>Texas Instruments, 2006&#8211;2007.</b> Firmware Engineer. <br>
    <b>Prime Minister's Office (Israel), 2001&#8211;2005.</b> Project Lead, Firmware/Software Engineer.
  </div>
  -->
    
  <div class="clear"></div>

  <h2 id="service">Service</h2>
  <div>

    <table>
      <tr>
        <td class="service" rowspan="5">SIGGRAPH</td>
        <td class="service">Technical Papers Committee 2022<strong>&ndash;</strong>2023</td>
      </tr>
      <tr>
        <td class="service">Technical Papers Reviewer 2015<strong>&ndash;</strong>2017, 2019<strong>&ndash;</strong>2024</td>
      </tr>
      <tr>
        <td class="service">Posters Adjunct Committee 2017</td>
      </tr>
      <tr>
        <td class="service">General Program Reviewer 2016</td>
      </tr>
      <tr>
        <td class="service">Posters Reviewer 2016</td>
      </tr>

      <tr>
        <td class="service" rowspan="2">SIGGRAPH Asia</td>
        <td class="service">Technical Papers Committee 2024</td>
      </tr>
      <tr>
        <td class="service">Reviewer 2018<strong>&ndash;</strong>2023</td>
      </tr>

      <tr>
        <td class="service">Israel Science Foundation (ISF)</td>
        <td class="service">Reviewer 2022<strong>&ndash;</strong>2023</td>
      </tr>

      <!-- Winter Conference on Applications of Computer Vision -->
      <tr>
        <td class="service">WACV</td>
        <td class="service">Reviewer 2023</td>
      </tr>

      <tr>
        <td class="service">CVPR</td>
        <td class="service">Reviewer 2017<strong>&ndash;</strong>2020 (<a href="https://cvpr2020.thecvf.com/reviewer-acknowledgements" target="_blank" rel="noopener">outstanding reviewer</a>), 2022
      </tr>

      <tr>
        <td class="service">ICLR</td>
        <td class="service">Reviewer 2022</td>
      </tr>

      <tr>
        <td class="service">UIST</td>
        <td class="service">Reviewer 2021<strong>&ndash;</strong>2022</td>
      </tr>

      <tr>
        <td class="service">Transactions on Visualization and Computer Graphics</td>
        <td class="service">Reviewer 2022</td>
      </tr>

      <tr>
        <td class="service">ICCV</td>
        <td class="service">Reviewer 2017, 2021</td>
      </tr>

      <!-- Transactions on Pattern Analysis and Machine Intelligence -->
      <tr>
        <td class="service">TPAMI</td>
        <td class="service">Reviewer 2021</td>
      </tr>

      <tr>
        <td class="service">ECCV</td>
        <td class="service">Reviewer 2018, 2020</td>
      </tr>

      <tr>
        <td class="service">Computer Graphics Forum</td>
        <td class="service">Reviewer 2015<strong>&ndash;</strong>2016, 2019</td>
      </tr>

      <tr>
        <td class="service">Transactions on Graphics</td>
        <td class="service">Reviewer 2016, 2018</td>
      </tr>

      <tr>
        <td class="service">Transactions on Applied Perception</td>
        <td class="service">Reviewer 2018</td>
      </tr>

      <tr>
        <td class="service">International Journal of Human-Computer Interaction</td>
        <td class="service">Reviewer 2018</td>
      </tr>

      <tr>
        <td class="service">The Visual Computer</td>
        <td class="service">Reviewer 2017</td>
      </tr>

      <tr>
        <td class="service">Transactions on Multimedia</td>
        <td class="service">Reviewer 2017</td>
      </tr>

      <tr>
        <td class="service">Transactions on Image Processing</td>
        <td class="service">Reviewer 2016</td>
      </tr>

    </table>
  </div>

  <div class="clear"></div>

  <h2 id="students">Students</h2>
  <div>
    <a href="https://omriavrahami.com/" target="_blank" rel="noopener">Omri Avrahami</a> (with <a href="https://www.cs.huji.ac.il/~danix/" target="_blank" rel="noopener">Dani Lischinski</a>). <br>
    <a href="https://www.linkedin.com/in/shimon-malnick-1b8404125/" target="_blank" rel="noopener">Shimon Malnick</a> (with <a href="https://www.eng.tau.ac.il/~avidan/" target="_blank" rel="noopener">Shai Avidan</a>). <br>
    <a href="https://www.linkedin.com/in/serge2020/" target="_blank" rel="noopener">Serge Sinitsa</a>. <br>
    <a href="https://www.linkedin.com/in/rotem-shalev/" target="_blank" rel="noopener">Rotem Shalev</a> (with <a href="https://www.cs.tau.ac.il/~amberman/" target="_blank" rel="noopener">Amit Bermano</a>). <br>
    <a href="https://www.linkedin.com/in/dan-botchan-b7a184141/" target="_blank" rel="noopener">Dan Botchan</a> (with <a href="https://faculty.runi.ac.il/toky/" target="_blank" rel="noopener">Yacov Hel-Or</a>). <br>
    <a href="#" target="_blank" rel="noopener">Omer David</a>. <br>
    <a href="https://www.linkedin.com/in/mogy" target="_blank" rel="noopener">Almog Friedlander</a>. <br>
    <a href="https://www.linkedin.com/in/eyal-michaeli-807b75151" target="_blank" rel="noopener">Eyal Michaeli</a>. <br>
    <a href="https://www.linkedin.com/in/galalmog/" target="_blank" rel="noopener">Gal Almog</a> (with <a href="https://faculty.runi.ac.il/arik/site/index.asp" target="_blank" rel="noopener">Ariel Shamir</a>). <br>
    <br>
    <p>Graduated:</p>
    <a href="#" target="_blank" rel="noopener">Eran Levin</a> (with <a href="https://www.cs.tau.ac.il/~amir1/" target="_blank" rel="noopener">Amir Averbuch</a>). <br>
    <a href="https://www.linkedin.com/in/karnieli/" target="_blank" rel="noopener">Asaf Karnieli</a> (with <a href="https://faculty.runi.ac.il/toky/" target="_blank" rel="noopener">Yacov Hel-Or</a>). <br>
    <a href="https://www.linkedin.com/in/dudi-dadon-48a72b193/" target="_blank" rel="noopener">Dudi Dadon</a> (with <a href="https://faculty.runi.ac.il/toky/" target="_blank" rel="noopener">Yacov Hel-Or</a>). <br>
    <a href="https://www.linkedin.com/in/daniel-anderson/" target="_blank" rel="noopener">Daniel Anderson</a> (with <a href="https://faculty.runi.ac.il/arik/site/index.asp" target="_blank" rel="noopener">Ariel Shamir</a>). <br>
    <a href="#" target="_blank" rel="noopener">Or Nachmias</a> (with <a href="https://faculty.runi.ac.il/arik/site/index.asp" target="_blank" rel="noopener">Ariel Shamir</a>). <br>
    <a href="#" target="_blank" rel="noopener">Gili Knafo</a>. <br>
    <a href="#" target="_blank" rel="noopener">Bar Cohen</a>. <br>
  </div>

  <div class="clear"></div>

</div>

  <footer>
    <a href="https://scholar.google.com/citations?user=YZcVsRMAAAAJ" target="_blank" rel="noopener" aria-label="google scholar">
      <i class="ai ai-google-scholar ai-2x"></i>
    </a>

    <a href="https://www.semanticscholar.org/author/Ohad-Fried/2416503" target="_blank" rel="noopener" aria-label="semantic scholar">
      <i class="ai ai-semantic-scholar ai-2x"></i>
    </a>

    <a href="https://www.youtube.com/channel/UCEvgUTPTeHd3PaF-9NZkNdg" target="_blank" rel="noopener" aria-label="youtube">
      <i class="fa-brands fa-youtube fa-2x" aria-hidden="true"></i>
    </a>

    <a href="https://twitter.com/ohadf" target="_blank" rel="noopener" aria-label="twitter">
      <i class="fa-brands fa-x-twitter fa-2x" aria-hidden="true"></i>
    </a>

    <a href="https://www.linkedin.com/in/ohadfried/" target="_blank" rel="noopener" aria-label="linkedin">
      <i class="fa-brands fa-linkedin fa-2x"></i>
    </a>

    <a href="mailto:ofried@runi.ac.il" aria-label="email">
      <i class="fa-regular fa-envelope fa-2x" aria-hidden="true"></i>
    </a>
  </footer>

</body>
</html>
